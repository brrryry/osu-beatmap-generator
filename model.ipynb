{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio2Map\n",
    "This is an encoder-decoder model based off of seq2seq.\n",
    "\n",
    "It takes in an audio file for the music as an mp3 and outputs a fully functional map for the hit rhythm game Osu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here, we transform the input to a Constant-Q spectrogram spanning C1 to roughly C7. Then, for training, we obtain the pkl file containing the output vector representing the target output map.\n",
    "\n",
    "We also obtain the difficulty for our target output to feed into the decoder, when deployed, this will be input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def convert_to_spectrogram(filename):\n",
    "\ttry:\n",
    "\t\ttargetSampleRate = 11025\n",
    "\t\ty, sr = librosa.load(filename, sr=targetSampleRate)\n",
    "\t\tC = np.abs(librosa.cqt(y, sr=targetSampleRate, n_bins=84, bins_per_octave=12))\n",
    "\t\tS = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\t\t#plot the spectrogram\n",
    "\t\t\n",
    "\t\t'''plt.figure(figsize=(12, 4))\n",
    "\t\tlibrosa.display.specshow(S, sr=targetSampleRate, x_axis='time', y_axis='cqt_note')\n",
    "\t\tplt.colorbar(format='%+2.0f dB')\n",
    "\t\tplt.title('Constant-Q power spectrogram')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()'''\n",
    "\t\treturn S\n",
    "\texcept:\n",
    "\t\ttsprint(\"ERROR: cannot convert to spectrogram. Removed file \" + filename + \".\")\n",
    "\n",
    "def get_pkl(filename):\n",
    "\ttry:\n",
    "\t\treturn pickle.load(open(filename, 'rb'))\n",
    "\texcept:\n",
    "\t\ttsprint(\"ERROR: .pkl file does not exist.\")\n",
    "\t\treturn -1\n",
    "\n",
    "def tsprint(s):\n",
    "\tprint(\"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \" + s)\n",
    "\n",
    "   \n",
    "def parse_difficulty(filename):\n",
    "\tif(not os.path.isfile(filename)):\n",
    "\t\ttsprint(\"ERROR: map file does not exist. Removing.\")\n",
    "\t\tos.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "\t\treturn -1\n",
    "\n",
    "\twith open(filename, \"r\") as f:\n",
    "\t\ttry:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\texcept:\n",
    "\t\t\ttsprint(\"ERROR: cannot read lines of .osu file.\")\n",
    "\n",
    "\n",
    "\tdifficulty = [-1,-1,-1,-1,-1,-1]\n",
    "\n",
    "\tfor line in lines:\n",
    "\t\t#difficulty\n",
    "\t\tif line.startswith(\"HPDrainRate\"): difficulty[0] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"CircleSize\"): difficulty[1] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"OverallDifficulty\"): difficulty[2] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"ApproachRate\"): difficulty[3] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"SliderMultiplier\"): difficulty[4] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"SliderTickRate\"): difficulty[5] = float(line.split(\":\", 1)[1])\n",
    "\t\telif not (line.startswith(\"[Difficulty]\")): break\n",
    "\n",
    "\t#check if all the difficulty stats are there\n",
    "\tfor val in difficulty:\n",
    "\t\tif val == -1:\n",
    "\t\t\ttsprint(\"ERROR: Not a valid osu! map due to insufficient stats. Removed file \" + filename + \".\")\n",
    "\t\t\tos.remove(filename)\n",
    "\t\t\treturn -1\n",
    "\n",
    "\n",
    "\treturn torch.tensor(difficulty)\n",
    "\n",
    "def load_data():\n",
    "\tinputs = []\n",
    "\tdiffs = []\n",
    "\ttargets = []\n",
    "\n",
    "\tcurr_length = 0\n",
    "\tcounter = 0\n",
    "\n",
    "\tif os.path.isfile(\"loaded_save.pkl\"):\n",
    "\t\tinputs, diffs, targets = pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "\t\tcurr_length = len(inputs)\n",
    "\n",
    "\n",
    "\tfor pickle_root, pickle_dirs, pickle_files in os.walk(\"pickles\"):\n",
    "\t\tfor pickle_file in pickle_files:\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tif counter < curr_length: continue\n",
    "\n",
    "\t\t\ttsprint(\"Parsing file \" + pickle_file)\n",
    "\t\t\tinputs.append(convert_to_spectrogram(os.path.join(\"audio/\", pickle_file.split(\"_\")[0] + \".mp3\")))\n",
    "\t\t\tdiffs.append(parse_difficulty(\"maps/\" + pickle_file.split(\".\")[0] + \".osu\"))\n",
    "\t\t\ttargets.append(get_pkl(\"pickles/\" + pickle_file))\n",
    "\n",
    "\t\t\tif counter % 100 == 0:\n",
    "\t\t\t\tpickle.dump([inputs, diffs, targets], open(\"loaded_save.pkl\", 'wb'))\n",
    "\t\t\t\ttsprint(\"Saved progress.\")\n",
    "\t\t\t\ttsprint(\"Parsed \" + str(counter) + \" files.\")\n",
    "\t\n",
    "\treturn inputs, diffs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-30 22:54:51] Parsing file 629136_6.pkl\n",
      "[2024-05-30 22:54:51] Parsing file 629136_4.pkl\n",
      "[2024-05-30 22:54:52] Parsing file 629136_5.pkl\n",
      "[2024-05-30 22:54:52] Parsing file 998593_1.pkl\n",
      "[2024-05-30 22:54:52] Parsing file 822575_1.pkl\n",
      "[2024-05-30 22:54:52] Parsing file 998593_0.pkl\n",
      "[2024-05-30 22:54:53] Parsing file 910271_1.pkl\n",
      "[2024-05-30 22:54:53] Parsing file 629136_0.pkl\n",
      "[2024-05-30 22:54:53] Parsing file 822575_2.pkl\n",
      "[2024-05-30 22:54:54] Parsing file 629136_1.pkl\n",
      "[2024-05-30 22:54:54] Parsing file 910271_3.pkl\n",
      "[2024-05-30 22:54:54] Parsing file 888479_2.pkl\n",
      "[2024-05-30 22:54:54] Parsing file 550491_0.pkl\n",
      "[2024-05-30 22:54:55] Parsing file 543109_2.pkl\n",
      "[2024-05-30 22:54:55] Parsing file 351153_0.pkl\n",
      "[2024-05-30 22:54:55] Parsing file 562857_0.pkl\n",
      "[2024-05-30 22:54:56] Parsing file 776808_0.pkl\n",
      "[2024-05-30 22:54:56] Parsing file 543109_0.pkl\n",
      "[2024-05-30 22:54:56] Parsing file 910271_0.pkl\n",
      "[2024-05-30 22:54:57] Parsing file 81254_0.pkl\n",
      "[2024-05-30 22:54:57] Parsing file 81254_1.pkl\n",
      "[2024-05-30 22:54:58] Parsing file 388089_0.pkl\n",
      "[2024-05-30 22:54:58] Parsing file 998593_2.pkl\n",
      "[2024-05-30 22:54:58] Parsing file 197148_0.pkl\n",
      "[2024-05-30 22:54:59] Parsing file 888479_0.pkl\n",
      "[2024-05-30 22:54:59] Parsing file 910271_2.pkl\n",
      "[2024-05-30 22:54:59] Parsing file 888479_1.pkl\n",
      "[2024-05-30 22:55:00] Parsing file 543109_1.pkl\n",
      "[2024-05-30 22:55:00] Parsing file 429296_0.pkl\n",
      "[2024-05-30 22:55:00] Parsing file 764338_0.pkl\n",
      "[2024-05-30 22:55:00] Parsing file 629136_2.pkl\n",
      "[2024-05-30 22:55:01] Parsing file 449901_0.pkl\n",
      "[2024-05-30 22:55:01] Parsing file 351153_2.pkl\n",
      "[2024-05-30 22:55:01] Parsing file 822575_0.pkl\n",
      "[2024-05-30 22:55:01] Parsing file 888479_3.pkl\n",
      "[2024-05-30 22:55:02] Parsing file 550491_1.pkl\n",
      "[2024-05-30 22:55:02] Parsing file 351153_1.pkl\n",
      "[2024-05-30 22:55:02] Parsing file 629136_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# Now get the data >:D\n",
    "inputs, diffs, targets = load_data() #pickle.load(open(\"loaded_save.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2195, 84])\n",
      "torch.Size([2195, 84])\n",
      "torch.Size([2195, 84])\n",
      "torch.Size([6633, 84])\n",
      "torch.Size([3289, 84])\n",
      "torch.Size([6633, 84])\n",
      "torch.Size([3316, 84])\n",
      "torch.Size([2195, 84])\n",
      "torch.Size([3289, 84])\n",
      "torch.Size([2195, 84])\n",
      "torch.Size([3316, 84])\n",
      "torch.Size([4180, 84])\n",
      "torch.Size([8356, 84])\n",
      "torch.Size([5715, 84])\n",
      "torch.Size([1933, 84])\n",
      "torch.Size([3030, 84])\n",
      "torch.Size([5386, 84])\n",
      "torch.Size([5715, 84])\n",
      "torch.Size([3316, 84])\n",
      "torch.Size([5627, 84])\n",
      "torch.Size([5627, 84])\n",
      "torch.Size([3737, 84])\n",
      "torch.Size([6633, 84])\n",
      "torch.Size([4559, 84])\n",
      "torch.Size([4180, 84])\n",
      "torch.Size([3316, 84])\n",
      "torch.Size([4180, 84])\n",
      "torch.Size([5715, 84])\n",
      "torch.Size([4245, 84])\n",
      "torch.Size([1919, 84])\n",
      "torch.Size([2195, 84])\n",
      "torch.Size([5212, 84])\n",
      "torch.Size([1933, 84])\n",
      "torch.Size([3289, 84])\n",
      "torch.Size([4180, 84])\n",
      "torch.Size([8356, 84])\n",
      "torch.Size([1933, 84])\n",
      "torch.Size([2195, 84])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "\tif inputs[i] is not None:\n",
    "\t\tinputs[i] = torch.tensor(inputs[i].T)\n",
    "\t\tdiffs[i] = torch.t(diffs[i])\n",
    "\t\ttargets[i] = torch.t(targets[i][0].to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_diffs, test_diffs, train_y, test_y = train_test_split(inputs, diffs, targets, test_size=0.1)\n",
    "train_x, val_x, train_diffs, val_diffs, train_y, val_y = train_test_split(train_x, train_diffs, train_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "- Given a song, we can generate a spectrogram\n",
    "- Take the spectrogram and produce a list of times (rythmic beats)\n",
    "\"\"\"\n",
    "# Encoder\n",
    "audio_dim = 84\n",
    "hidden_dim = 64\n",
    "\n",
    "# Use LSTM to predict the note timings of the song\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "num_features = 8\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(num_features + 6, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_out, encoder_hc, difficulty, target=None):\n",
    "        decoder_input = torch.zeros((1, num_features + 6))\n",
    "        decoder_hidden = encoder_hc\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(encoder_out.shape[1]):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target is not None:\n",
    "                decoder_input = torch.cat((target[i], torch.reshape(difficulty, (1, -1))), 1)\n",
    "            else:\n",
    "                print(decoder_output.shape)\n",
    "                decoder_input = torch.cat((decoder_output, torch.reshape(difficulty, (1, -1))), 1)\n",
    "        print(decoder_outputs.shape)\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "        \n",
    "    def forward_step(self, x, hc):\n",
    "        x, hc = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        return x, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "enc = Encoder(0.4)\n",
    "dec = Decoder(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def train_epoch(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc):\n",
    "    # For now, data is a tuple of (x, diff, y)\n",
    "    total_loss = 0\n",
    "    for sample in data:\n",
    "        x = sample[0]\n",
    "        diff = sample[1]\n",
    "        y = sample[2]\n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hc = encoder(x)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hc, diff)\n",
    "\n",
    "        loss = lossfunc(decoder_outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(data)\n",
    "\n",
    "def train(data, encoder, decoder, epochs=10, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    losshistory = []\n",
    "\n",
    "    lossfunc = nn.MSELoss()\n",
    "\n",
    "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
    "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(data, encoder, decoder, enc_opt, dec_opt, lossfunc)\n",
    "        curr_time = time.time()\n",
    "        losshistory.append(loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss} Time: {curr_time - start}\")\n",
    "    return losshistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 14, got 70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[292], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_diffs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[291], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, encoder, decoder, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m dec_opt \u001b[38;5;241m=\u001b[39m Adam(dec\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     curr_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m     losshistory\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[291], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc)\u001b[0m\n\u001b[1;32m     10\u001b[0m decoder_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m encoder_outputs, encoder_hc \u001b[38;5;241m=\u001b[39m encoder(x)\n\u001b[0;32m---> 13\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(decoder_outputs, y)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[289], line 17\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, encoder_out, encoder_hc, difficulty, target)\u001b[0m\n\u001b[1;32m     14\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(encoder_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 17\u001b[0m     decoder_output, decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[289], line 30\u001b[0m, in \u001b[0;36mDecoder.forward_step\u001b[0;34m(self, x, hc)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hc):\n\u001b[0;32m---> 30\u001b[0m     x, hc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, hc\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:907\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    904\u001b[0m             hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:821\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    817\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    818\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    819\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    820\u001b[0m                        ):\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    823\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    825\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:240\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 14, got 70"
     ]
    }
   ],
   "source": [
    "train_loss = train(list(zip(train_x, train_diffs, train_y)), enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions :O\n",
    "\n",
    "Do some fine tuning for the model as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio):\n",
    "\tstates = encoder.predict(audio)\n",
    "\t\n",
    "\ttarget_seq = np.zeros((1, 1, 84))\n",
    "\n",
    "\tdecoded_map = []\n",
    "\n",
    "\tfor i in range(audio.shape[0]):\n",
    "\t\toutput, h, c = decoder.predict([target_seq] + states)\n",
    "\t\t\n",
    "\t\tdecoded_map.append(output)\n",
    "\n",
    "\t\tstates = [h, c]\n",
    "\n",
    "\treturn decoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\tdecoded_map = decode_audio(test_x[i])\t\n",
    "\tprint(\"Actual: \")\n",
    "\tprint(test_y[i])\n",
    "\tprint(\"Predicted: \")\n",
    "\tprint(decoded_map)\n",
    "\tprint(keras.losses.MSE(test_y[i], decoded_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate that bish B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = np.zeros(targets.shape)\n",
    "decoder_targets[:, 0:-1] = decoder_targets[:, 1:]\n",
    "\n",
    "audio2map.compile(optimizer='adam', loss='mean_squared_error', metrics=['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "history = audio2map.fit([inputs, targets, diffs], decoder_targets, epochs=10, batch_size=32, validation_split=0.15)\n",
    "audio2map.save('audio2map_full.h5') # This may not work well, but just in case we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Loss: \")\n",
    "print(history.history['loss'])\n",
    "print(\"Final Accuracy: \")\n",
    "print(history.history['accuracy'])\n",
    "print(\"Final Precision: \")\n",
    "print(history.history['precision'])\n",
    "print(\"Final Recall: \")\n",
    "print(history.history['recall'])\n",
    "print(\"Final F1: \")\n",
    "print(history.history['f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
