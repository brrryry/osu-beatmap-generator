{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtB_iV9kFTh"
      },
      "source": [
        "# Audio2Map\n",
        "This is an encoder-decoder model based off of seq2seq.\n",
        "\n",
        "It takes in an audio file for the music as an mp3 and outputs a fully functional map for the hit rhythm game Osu!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F8rmUKpkFTi"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import torch\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j7JIKMQkFTj"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "num_features = 8\n",
        "STOP = torch.full((1, num_features), -1, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqPQSKcDkFTk"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Here, we transform the input to a Constant-Q spectrogram spanning C1 to roughly C7. Then, for training, we obtain the pkl file containing the output vector representing the target output map.\n",
        "\n",
        "We also obtain the difficulty for our target output to feed into the decoder, when deployed, this will be input from the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSXGo1phkFTl"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "def convert_to_spectrogram(filename):\n",
        "\ttry:\n",
        "\t\ttargetSampleRate = 11025\n",
        "\t\ty, sr = librosa.load(filename, sr=targetSampleRate)\n",
        "\t\tC = np.abs(librosa.cqt(y, sr=targetSampleRate, n_bins=84, bins_per_octave=12))\n",
        "\t\tS = librosa.amplitude_to_db(C, ref=np.max)\n",
        "\t\t#plot the spectrogram\n",
        "\n",
        "\t\t'''plt.figure(figsize=(12, 4))\n",
        "\t\tlibrosa.display.specshow(S, sr=targetSampleRate, x_axis='time', y_axis='cqt_note')\n",
        "\t\tplt.colorbar(format='%+2.0f dB')\n",
        "\t\tplt.title('Constant-Q power spectrogram')\n",
        "\t\tplt.tight_layout()\n",
        "\t\tplt.show()'''\n",
        "\t\treturn S\n",
        "\texcept:\n",
        "\t\ttsprint(\"ERROR: cannot convert to spectrogram. Removed file \" + filename + \".\")\n",
        "\t\tos.remove(filename) #remove audio file\n",
        "\t\tfname = filename.split('/')[1]\n",
        "\t\t# Remove all maps with invalid audio\n",
        "\t\tfor f in [name for name in os.listdir(\"maps/\") if os.path.isfile(os.path.join(\"maps/\", name))]:\n",
        "\t\t\tif fname in f:\n",
        "\t\t\t\tos.remove(os.path.join(\"maps/\", f))\n",
        "\t\t# Remove all pkls with invalid audio\n",
        "\t\tfor f in [name for name in os.listdir(\"pickles/\") if os.path.isfile(os.path.join(\"pickles/\", name))]:\n",
        "\t\t\tif fname in f:\n",
        "\t\t\t\tos.remove(os.path.join(\"pickles/\", f))\n",
        "\n",
        "def get_pkl(filename):\n",
        "\ttry:\n",
        "\t\treturn pickle.load(open(filename, 'rb'))\n",
        "\texcept:\n",
        "\t\ttsprint(\"ERROR: .pkl file does not exist.\")\n",
        "\t\treturn -1\n",
        "\n",
        "def tsprint(s):\n",
        "\tprint(\"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \" + s)\n",
        "\n",
        "def parse_difficulty(filename):\n",
        "\tif(not os.path.isfile(filename)):\n",
        "\t\ttsprint(\"ERROR: map file does not exist. Removing.\")\n",
        "\t\tos.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
        "\t\treturn -1\n",
        "\n",
        "\twith open(filename, \"r\") as f:\n",
        "\t\ttry:\n",
        "\t\t\tlines = f.readlines()\n",
        "\t\texcept:\n",
        "\t\t\ttsprint(\"ERROR: cannot read lines of .osu file.\")\n",
        "\n",
        "\n",
        "\tdifficulty = [-1,-1,-1,-1,-1,-1]\n",
        "\n",
        "\tfor line in lines:\n",
        "\t\t#difficulty\n",
        "\t\tif line.startswith(\"HPDrainRate\"): difficulty[0] = float(line.split(\":\", 1)[1])\n",
        "\t\telif line.startswith(\"CircleSize\"): difficulty[1] = float(line.split(\":\", 1)[1])\n",
        "\t\telif line.startswith(\"OverallDifficulty\"): difficulty[2] = float(line.split(\":\", 1)[1])\n",
        "\t\telif line.startswith(\"ApproachRate\"): difficulty[3] = float(line.split(\":\", 1)[1])\n",
        "\t\telif line.startswith(\"SliderMultiplier\"): difficulty[4] = float(line.split(\":\", 1)[1])\n",
        "\t\telif line.startswith(\"SliderTickRate\"): difficulty[5] = float(line.split(\":\", 1)[1])\n",
        "\t\telif not (line.startswith(\"[Difficulty]\")): break\n",
        "\n",
        "\t#check if all the difficulty stats are there\n",
        "\tfor val in difficulty:\n",
        "\t\tif val == -1:\n",
        "\t\t\ttsprint(\"ERROR: Not a valid osu! map due to insufficient stats. Removed file \" + filename + \".\")\n",
        "\t\t\tos.remove(filename)\n",
        "\t\t\treturn -1\n",
        "\n",
        "\n",
        "\treturn torch.tensor(difficulty)\n",
        "\n",
        "# TODO: Deprecate\n",
        "def load_data():\n",
        "\tinputs = []\n",
        "\tdiffs = []\n",
        "\ttargets = []\n",
        "\n",
        "\tcurr_length = 0\n",
        "\tcounter = 0\n",
        "\n",
        "\tif os.path.isfile(\"loaded_save.pkl\"):\n",
        "\t\tinputs, diffs, targets = pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
        "\t\tcurr_length = len(inputs)\n",
        "\n",
        "\n",
        "\tfor pickle_root, pickle_dirs, pickle_files in os.walk(\"pickles\"):\n",
        "\t\tfor pickle_file in pickle_files:\n",
        "\t\t\tcounter += 1\n",
        "\t\t\tif counter < curr_length: continue\n",
        "\n",
        "\t\t\ttsprint(\"Parsing file \" + pickle_file)\n",
        "\t\t\tinputs.append(convert_to_spectrogram(os.path.join(\"audio/\", pickle_file.split(\"_\")[0] + \".mp3\")))\n",
        "\t\t\tdiffs.append(parse_difficulty(\"maps/\" + pickle_file.split(\".\")[0] + \".osu\"))\n",
        "\t\t\ttargets.append(get_pkl(\"pickles/\" + pickle_file))\n",
        "\n",
        "\t\t\tif counter % 10 == 0:\n",
        "\t\t\t\tpickle.dump([inputs, diffs, targets], open(\"loaded_save.pkl\", 'wb'))\n",
        "\t\t\t\ttsprint(\"Saved progress.\")\n",
        "\t\t\t\ttsprint(\"Parsed \" + str(counter) + \" files.\")\n",
        "\n",
        "\treturn inputs, diffs, targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Audio2Map(Dataset):\n",
        "     def __init__(self, input_dir, maps_dir, target_dir):\n",
        "        self.in_dir = input_dir\n",
        "        self.maps_dir = maps_dir\n",
        "        self.tar_dir = target_dir\n",
        "     def __len__(self):\n",
        "        return len([name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))])\n",
        "     def __getitem__(self, idx):\n",
        "        # use the listdir() index 5Head\n",
        "        # Get the current map name w/o .osu\n",
        "        files = [name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))]\n",
        "        currfile = files[idx][:-4]\n",
        "        spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
        "        while(type(spec) == type(None)):\n",
        "            idx += 1\n",
        "            currfile = files[idx][:-4]\n",
        "            spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
        "        input = torch.tensor(spec.T).float()\n",
        "        diff = torch.t(parse_difficulty(os.path.join(self.maps_dir, currfile + \".osu\"))).float()\n",
        "        out = get_pkl(os.path.join(self.tar_dir, currfile + \".pkl\"))[0].to_dense().float()\n",
        "        out = torch.cat((out, STOP), 0)\n",
        "        return input, diff, out"
      ],
      "metadata": {
        "id": "eyIAfiKKfizM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oru0NiOFkFTm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "# Now get the data >:D\n",
        "#inputs, diffs, targets = load_data() #pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
        "a2m_data = Audio2Map(\"audio/\", \"maps/\", \"pickles/\")\n",
        "\n",
        "test_split = 0.2\n",
        "train_data, test_data = random_split(a2m_data, [1-test_split, test_split])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIdMBhp5kFTn"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Must sample individually due to each input and output being different sizes :(\n",
        "train_dl = DataLoader(train_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
        "test_dl = DataLoader(test_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
        "# Output from dataloader is a list of size 3 containing a single input, difficulty, and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxIynZFfkFTo"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\"\"\"\n",
        "- Given a song, we can generate a spectrogram\n",
        "- Take the spectrogram and produce a list of times (rythmic beats)\n",
        "\"\"\"\n",
        "# Encoder\n",
        "audio_dim = 84\n",
        "hidden_dim = 64\n",
        "\n",
        "# Use LSTM to predict the note timings of the song\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True, bidirectional=True, device=device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out, hidden = self.lstm(x)\n",
        "        out = self.dropout(out)\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTIK8gcRkFTp"
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "from math import floor\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(num_features + 6, hidden_dim, num_layers=2, batch_first=True, device=device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hiddenfc = nn.Linear(hidden_dim, hidden_dim//2, device=device)\n",
        "        self.outputfc = nn.Linear(hidden_dim//2, num_features, device=device)\n",
        "\n",
        "    def forward(self, encoder_out, encoder_hc, difficulty, target=None):\n",
        "        decoder_input = torch.zeros((1, num_features + 6), device=device)\n",
        "        decoder_hidden = encoder_hc\n",
        "        decoder_outputs = []\n",
        "\n",
        "        prev_percent = 0\n",
        "        currStop = torch.cat((STOP.to(device), difficulty.unsqueeze(0)), 1)\n",
        "\n",
        "        while(not torch.equal(decoder_input, currStop)):\n",
        "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target is not None:\n",
        "                curr_percent = floor(((len(decoder_outputs)+1)/target.shape[0])*100)\n",
        "                if curr_percent > prev_percent:\n",
        "                    prev_percent = curr_percent\n",
        "                    print(f\"Training...{curr_percent}%\")\n",
        "\n",
        "                i = len(decoder_outputs)-1\n",
        "                decoder_input = torch.cat((target[i], difficulty), 0).unsqueeze(0)\n",
        "            else:\n",
        "                if (len(decoder_outputs)+1) % 100 == 0\n",
        "                    print(f\"Timestep: {len(decoder_outputs)+1}\")\n",
        "                decoder_input = torch.cat((decoder_output, difficulty.unsqueeze(0)), 1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
        "        return decoder_outputs, decoder_hidden, None\n",
        "\n",
        "    def forward_step(self, x, hc):\n",
        "        x, hc = self.lstm(x, hc)\n",
        "        drp = self.dropout(x)\n",
        "        hidden = self.hiddenfc(drp)\n",
        "        out = self.outputfc(hidden)\n",
        "        return out, hc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dzeN2TekFTp"
      },
      "source": [
        "## Training Time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFNJmcMfkFTp"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "# Create models and offload to GPU for processing, if available\n",
        "enc = Encoder(0.4).to(device)\n",
        "dec = Decoder(0.4).to(device)\n",
        "\n",
        "def print_model_size(model):\n",
        "    param_size = 0\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "    buffer_size = 0\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
        "\n",
        "print_model_size(enc)\n",
        "print_model_size(dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbZq22_XkFTq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train_epoch(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc):\n",
        "    # For now, data is a tuple of (x, diff, y)\n",
        "    total_loss = 0\n",
        "    for i, sample in enumerate(data):\n",
        "        tsprint(f\"Current sample {i+1}\")\n",
        "        # Put data onto the GPU if available, otherwise its just on cpu unlucky\n",
        "        x = sample[0].to(device)\n",
        "        diff = sample[1].to(device)\n",
        "        y = sample[2].to(device)\n",
        "\n",
        "        encoder_opt.zero_grad()\n",
        "        decoder_opt.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hc = encoder(x)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hc, diff, target=y)\n",
        "\n",
        "        loss = lossfunc(decoder_outputs, y)\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_opt.step()\n",
        "        decoder_opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss/len(data)\n",
        "\n",
        "def train(data, encoder, decoder, epochs=10, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    losshistory = []\n",
        "\n",
        "    lossfunc = nn.MSELoss()\n",
        "\n",
        "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
        "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
        "    for epoch in range(epochs):\n",
        "        tsprint(f\"Epoch: {epoch+1}\")\n",
        "        loss = train_epoch(data, encoder, decoder, enc_opt, dec_opt, lossfunc)\n",
        "        curr_time = time.time()\n",
        "        losshistory.append(loss)\n",
        "        print(f\"Loss: {loss} Time: {curr_time - start}\")\n",
        "    return losshistory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mem_req(enc, dec):\n",
        "    # register forward hooks to check intermediate activation size\n",
        "    acts = []\n",
        "    for name, module in enc.named_modules():\n",
        "        if name == 'classifier' or name == 'features':\n",
        "            continue\n",
        "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
        "    for name, module in dec.named_modules():\n",
        "        if name == 'classifier' or name == 'features':\n",
        "            continue\n",
        "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
        "\n",
        "    # execute single training step\n",
        "    X, diff, y_true = next(iter(train_dl))\n",
        "    # Forward pass\n",
        "    y_hat, hc = enc(X[:10000])\n",
        "    y_hat, _, _ = dec(y_hat, hc, diff, target=y_true[-10000:])\n",
        "    loss = nn.MSELoss()(y_hat, y_true[-10000:])\n",
        "    # Backward pass\n",
        "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
        "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
        "    enc_opt.zero_grad()\n",
        "    dec_opt.zero_grad()\n",
        "    loss.backward()\n",
        "    enc_opt.step()\n",
        "    dec_opt.step()\n",
        "\n",
        "    # approximate memory requirements\n",
        "    model_param_size = sum([p.nelement() for p in enc.parameters()]) + sum([p.nelement() for p in dec.parameters()])\n",
        "    grad_size = model_param_size\n",
        "    batch_size = reduce((lambda x, y: x * y), X.shape)\n",
        "    optimizer_size = sum([p.nelement() for p in enc_opt.param_groups[0]['params']]) + sum([p.nelement() for p in dec_opt.param_groups[0]['params']])\n",
        "    act_size = sum([a.nelement() for a in acts])\n",
        "\n",
        "    total_nb_elements = model_param_size + grad_size + batch_size + optimizer_size + act_size\n",
        "    total_mb = total_nb_elements * 4 / 1024**2\n",
        "    print(total_mb)\n",
        "#get_mem_req(enc, dec)"
      ],
      "metadata": {
        "id": "ELEk-RkK0TfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1x1UlpN6kFT2"
      },
      "outputs": [],
      "source": [
        "train_loss = train(train_dl, enc, dec, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(enc.state_dict(), 'encoder.pth')\n",
        "torch.save(dec.state_dict(), 'decoder.pth')"
      ],
      "metadata": {
        "id": "Xs3Ued2dvTbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s0vsJxykFT2"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(train_loss)), train_loss, 'b', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_dFxINnkFT2"
      },
      "source": [
        "## Make some predictions :O\n",
        "\n",
        "Do some fine tuning for the model as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc_k9kMdkFT2"
      },
      "outputs": [],
      "source": [
        "def decode_audio(encoder, decoder, audio, diff):\n",
        "\tenc_out, states = encoder(audio)\n",
        "\tout, _, _ = decoder(enc_out, states, diff)\n",
        "\n",
        "\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3b-0iYjkFT2"
      },
      "outputs": [],
      "source": [
        "test_loss = 0\n",
        "num_samples = 1\n",
        "\n",
        "for i, sample in enumerate(test_dl):\n",
        "\tx = sample[0].to(device)\n",
        "\tdiff = sample[1].to(device)\n",
        "\ty = sample[2].to(device)\n",
        "\tdecoded_map = decode_audio(enc, dec, x, diff)\n",
        "\tprint(\"Actual: \")\n",
        "\tprint(y)\n",
        "\tprint(\"Predicted: \")\n",
        "\tprint(decoded_map)\n",
        "\tcurr_loss = nn.MSELoss()(y, decoded_map)\n",
        "\tprint(curr_loss)\n",
        "\ttest_loss += curr_loss\n",
        "test_loss /= num_samples\n",
        "print(f\"Average Testing Loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEvlq_f-kFT2"
      },
      "source": [
        "## Evaluate that bish B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-zjvlU6kFT3"
      },
      "outputs": [],
      "source": [
        "full_enc = Encoder(0.4).to(device)\n",
        "full_dec = Decoder(0.4).to(device)\n",
        "a2m_dl = DataLoader(a2m_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
        "\n",
        "full_train_loss = train(a2m_dl, enc, dec, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(len(full_train_loss)), full_train_loss, 'b', label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c25IBt1rci8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_test_loss = 0\n",
        "num_samples = 1\n",
        "\n",
        "for i, sample in enumerate(test_dl):\n",
        "\tx = sample[0].to(device)\n",
        "\tdiff = sample[1].to(device)\n",
        "\ty = sample[2].to(device)\n",
        "\tdecoded_map = decode_audio(enc, dec, x, diff)\n",
        "\tprint(\"Actual: \")\n",
        "\tprint(y)\n",
        "\tprint(\"Predicted: \")\n",
        "\tprint(decoded_map)\n",
        "\tcurr_loss = nn.MSELoss()(y, decoded_map)\n",
        "\tprint(curr_loss)\n",
        "\tfull_test_loss += curr_loss\n",
        "full_test_loss /= num_samples\n",
        "print(f\"Average Testing Loss: {full_test_loss}\")"
      ],
      "metadata": {
        "id": "g4CO4F-EcomA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}