{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio2Map\n",
    "This is an encoder-decoder model based off of seq2seq.\n",
    "\n",
    "It takes in an audio file for the music as an mp3 and outputs a fully functional map for the hit rhythm game Osu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here, we transform the input to a Constant-Q spectrogram spanning C1 to roughly C7. Then, for training, we obtain the pkl file containing the output vector representing the target output map.\n",
    "\n",
    "We also obtain the difficulty for our target output to feed into the decoder, when deployed, this will be input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def convert_to_spectrogram(filename):\n",
    "\ttry:\n",
    "\t\ttargetSampleRate = 11025\n",
    "\t\ty, sr = librosa.load(filename, sr=targetSampleRate)\n",
    "\t\tC = np.abs(librosa.cqt(y, sr=targetSampleRate, n_bins=84, bins_per_octave=12))\n",
    "\t\tS = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\t\t#plot the spectrogram\n",
    "\t\t\n",
    "\t\t'''plt.figure(figsize=(12, 4))\n",
    "\t\tlibrosa.display.specshow(S, sr=targetSampleRate, x_axis='time', y_axis='cqt_note')\n",
    "\t\tplt.colorbar(format='%+2.0f dB')\n",
    "\t\tplt.title('Constant-Q power spectrogram')\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()'''\n",
    "\t\treturn S\n",
    "\texcept:\n",
    "\t\ttsprint(\"ERROR: cannot convert to spectrogram. Removed file \" + filename + \".\")\n",
    "\n",
    "def get_pkl(filename):\n",
    "\ttry:\n",
    "\t\treturn pickle.load(open(filename, 'rb'))\n",
    "\texcept:\n",
    "\t\ttsprint(\"ERROR: .pkl file does not exist.\")\n",
    "\t\treturn -1\n",
    "\n",
    "def tsprint(s):\n",
    "\tprint(\"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \" + s)\n",
    "\n",
    "   \n",
    "def parse_difficulty(filename):\n",
    "\tif(not os.path.isfile(filename)):\n",
    "\t\ttsprint(\"ERROR: map file does not exist. Removing.\")\n",
    "\t\tos.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "\t\treturn -1\n",
    "\n",
    "\twith open(filename, \"r\") as f:\n",
    "\t\ttry:\n",
    "\t\t\tlines = f.readlines()\n",
    "\t\texcept:\n",
    "\t\t\ttsprint(\"ERROR: cannot read lines of .osu file.\")\n",
    "\n",
    "\n",
    "\tdifficulty = [-1,-1,-1,-1,-1,-1]\n",
    "\n",
    "\tfor line in lines:\n",
    "\t\t#difficulty\n",
    "\t\tif line.startswith(\"HPDrainRate\"): difficulty[0] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"CircleSize\"): difficulty[1] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"OverallDifficulty\"): difficulty[2] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"ApproachRate\"): difficulty[3] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"SliderMultiplier\"): difficulty[4] = float(line.split(\":\", 1)[1])\n",
    "\t\telif line.startswith(\"SliderTickRate\"): difficulty[5] = float(line.split(\":\", 1)[1])\n",
    "\t\telif not (line.startswith(\"[Difficulty]\")): break\n",
    "\n",
    "\t#check if all the difficulty stats are there\n",
    "\tfor val in difficulty:\n",
    "\t\tif val == -1:\n",
    "\t\t\ttsprint(\"ERROR: Not a valid osu! map due to insufficient stats. Removed file \" + filename + \".\")\n",
    "\t\t\tos.remove(filename)\n",
    "\t\t\treturn -1\n",
    "\n",
    "\n",
    "\treturn torch.tensor(difficulty)\n",
    "\n",
    "def load_data():\n",
    "\tinputs = []\n",
    "\tdiffs = []\n",
    "\ttargets = []\n",
    "\n",
    "\tcurr_length = 0\n",
    "\tcounter = 0\n",
    "\n",
    "\tif os.path.isfile(\"loaded_save.pkl\"):\n",
    "\t\tinputs, diffs, targets = pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "\t\tcurr_length = len(inputs)\n",
    "\n",
    "\n",
    "\tfor pickle_root, pickle_dirs, pickle_files in os.walk(\"pickles\"):\n",
    "\t\tfor pickle_file in pickle_files:\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tif counter < curr_length: continue\n",
    "\n",
    "\t\t\ttsprint(\"Parsing file \" + pickle_file)\n",
    "\t\t\tinputs.append(convert_to_spectrogram(os.path.join(\"audio/\", pickle_file.split(\"_\")[0] + \".mp3\")))\n",
    "\t\t\tdiffs.append(parse_difficulty(\"maps/\" + pickle_file.split(\".\")[0] + \".osu\"))\n",
    "\t\t\ttargets.append(get_pkl(\"pickles/\" + pickle_file))\n",
    "\n",
    "\t\t\tif counter % 100 == 0:\n",
    "\t\t\t\tpickle.dump([inputs, diffs, targets], open(\"loaded_save.pkl\", 'wb'))\n",
    "\t\t\t\ttsprint(\"Saved progress.\")\n",
    "\t\t\t\ttsprint(\"Parsed \" + str(counter) + \" files.\")\n",
    "\t\n",
    "\treturn inputs, diffs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-31 18:23:59] Parsing file 629136_6.pkl\n",
      "[2024-05-31 18:24:00] Parsing file 629136_4.pkl\n",
      "[2024-05-31 18:24:00] Parsing file 629136_5.pkl\n",
      "[2024-05-31 18:24:00] Parsing file 998593_1.pkl\n",
      "[2024-05-31 18:24:00] Parsing file 822575_1.pkl\n",
      "[2024-05-31 18:24:01] Parsing file 998593_0.pkl\n",
      "[2024-05-31 18:24:01] Parsing file 910271_1.pkl\n",
      "[2024-05-31 18:24:01] Parsing file 629136_0.pkl\n",
      "[2024-05-31 18:24:02] Parsing file 822575_2.pkl\n",
      "[2024-05-31 18:24:02] Parsing file 629136_1.pkl\n",
      "[2024-05-31 18:24:02] Parsing file 910271_3.pkl\n",
      "[2024-05-31 18:24:02] Parsing file 888479_2.pkl\n",
      "[2024-05-31 18:24:03] Parsing file 550491_0.pkl\n",
      "[2024-05-31 18:24:03] Parsing file 543109_2.pkl\n",
      "[2024-05-31 18:24:04] Parsing file 351153_0.pkl\n",
      "[2024-05-31 18:24:04] Parsing file 562857_0.pkl\n",
      "[2024-05-31 18:24:04] Parsing file 776808_0.pkl\n",
      "[2024-05-31 18:24:04] Parsing file 543109_0.pkl\n",
      "[2024-05-31 18:24:05] Parsing file 910271_0.pkl\n",
      "[2024-05-31 18:24:05] Parsing file 81254_0.pkl\n",
      "[2024-05-31 18:24:05] Parsing file 81254_1.pkl\n",
      "[2024-05-31 18:24:06] Parsing file 388089_0.pkl\n",
      "[2024-05-31 18:24:06] Parsing file 998593_2.pkl\n",
      "[2024-05-31 18:24:07] Parsing file 197148_0.pkl\n",
      "[2024-05-31 18:24:07] Parsing file 888479_0.pkl\n",
      "[2024-05-31 18:24:07] Parsing file 910271_2.pkl\n",
      "[2024-05-31 18:24:08] Parsing file 888479_1.pkl\n",
      "[2024-05-31 18:24:08] Parsing file 543109_1.pkl\n",
      "[2024-05-31 18:24:08] Parsing file 429296_0.pkl\n",
      "[2024-05-31 18:24:09] Parsing file 764338_0.pkl\n",
      "[2024-05-31 18:24:09] Parsing file 629136_2.pkl\n",
      "[2024-05-31 18:24:09] Parsing file 449901_0.pkl\n",
      "[2024-05-31 18:24:09] Parsing file 351153_2.pkl\n",
      "[2024-05-31 18:24:09] Parsing file 822575_0.pkl\n",
      "[2024-05-31 18:24:10] Parsing file 888479_3.pkl\n",
      "[2024-05-31 18:24:10] Parsing file 550491_1.pkl\n",
      "[2024-05-31 18:24:11] Parsing file 351153_1.pkl\n",
      "[2024-05-31 18:24:11] Parsing file 629136_3.pkl\n"
     ]
    }
   ],
   "source": [
    "# Now get the data >:D\n",
    "inputs, diffs, targets = load_data() #pickle.load(open(\"loaded_save.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = torch.full((1, num_features), -1, dtype=torch.float32)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "\tif inputs[i] is not None:\n",
    "\t\tinputs[i] = torch.tensor(inputs[i].T).float()\n",
    "\t\tdiffs[i] = torch.t(diffs[i]).float()\n",
    "\t\ttargets[i] = targets[i][0].to_dense().float()\n",
    "\t\ttargets[i] = torch.cat((targets[i], STOP), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_diffs, test_diffs, train_y, test_y = train_test_split(inputs, diffs, targets, test_size=0.1)\n",
    "train_x, val_x, train_diffs, val_diffs, train_y, val_y = train_test_split(train_x, train_diffs, train_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "- Given a song, we can generate a spectrogram\n",
    "- Take the spectrogram and produce a list of times (rythmic beats)\n",
    "\"\"\"\n",
    "# Encoder\n",
    "audio_dim = 84\n",
    "hidden_dim = 64\n",
    "\n",
    "# Use LSTM to predict the note timings of the song\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True, bidirectional=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "num_features = 8\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(num_features + 6, hidden_dim, num_layers=2, batch_first=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hiddenfc = nn.Linear(hidden_dim, hidden_dim//2, device=device)\n",
    "        self.outputfc = nn.Linear(hidden_dim//2, num_features, device=device)\n",
    "\n",
    "    def forward(self, encoder_out, encoder_hc, difficulty, target=None):\n",
    "        decoder_input = torch.zeros((1, num_features + 6))\n",
    "        decoder_hidden = encoder_hc\n",
    "        decoder_outputs = []\n",
    "\n",
    "        while(not torch.equal(decoder_input, STOP)):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target is not None:\n",
    "                i = len(decoder_outputs)-1\n",
    "                decoder_input = torch.cat((target[i], difficulty), 0).unsqueeze(0)\n",
    "            else:\n",
    "                decoder_input = torch.cat((decoder_output, difficulty.unsqueeze(0)), 1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "        \n",
    "    def forward_step(self, x, hc):\n",
    "        x, hc = self.lstm(x, hc)\n",
    "        drp = self.dropout(x)\n",
    "        hidden = self.hiddenfc(drp)\n",
    "        out = self.outputfc(hidden)\n",
    "        return out, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "# Create models and offload to GPU for processing, if available\n",
    "enc = Encoder(0.4).to(device)\n",
    "dec = Decoder(0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def train_epoch(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc):\n",
    "    # For now, data is a tuple of (x, diff, y)\n",
    "    total_loss = 0\n",
    "    for sample in data:\n",
    "        # Put data onto the GPU if available, otherwise its just on cpu unlucky\n",
    "        x = sample[0].to(device)\n",
    "        diff = sample[1].to(device)\n",
    "        y = sample[2].to(device)\n",
    "        \n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hc = encoder(x)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hc, diff, target=y)\n",
    "\n",
    "        loss = lossfunc(decoder_outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss/len(data)\n",
    "\n",
    "def train(data, encoder, decoder, epochs=10, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    losshistory = []\n",
    "\n",
    "    lossfunc = nn.MSELoss()\n",
    "\n",
    "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
    "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(data, encoder, decoder, enc_opt, dec_opt, lossfunc)\n",
    "        curr_time = time.time()\n",
    "        losshistory.append(loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss} Time: {curr_time - start}\")\n",
    "    return losshistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = train(list(zip(train_x, train_diffs, train_y)), enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions :O\n",
    "\n",
    "Do some fine tuning for the model as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(audio):\n",
    "\tstates = encoder.predict(audio)\n",
    "\t\n",
    "\ttarget_seq = np.zeros((1, 1, 84))\n",
    "\n",
    "\tdecoded_map = []\n",
    "\n",
    "\tfor i in range(audio.shape[0]):\n",
    "\t\toutput, h, c = decoder.predict([target_seq] + states)\n",
    "\t\t\n",
    "\t\tdecoded_map.append(output)\n",
    "\n",
    "\t\tstates = [h, c]\n",
    "\n",
    "\treturn decoded_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "\tdecoded_map = decode_audio(test_x[i])\t\n",
    "\tprint(\"Actual: \")\n",
    "\tprint(test_y[i])\n",
    "\tprint(\"Predicted: \")\n",
    "\tprint(decoded_map)\n",
    "\tprint(keras.losses.MSE(test_y[i], decoded_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate that bish B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets = np.zeros(targets.shape)\n",
    "decoder_targets[:, 0:-1] = decoder_targets[:, 1:]\n",
    "\n",
    "audio2map.compile(optimizer='adam', loss='mean_squared_error', metrics=['loss', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "history = audio2map.fit([inputs, targets, diffs], decoder_targets, epochs=10, batch_size=32, validation_split=0.15)\n",
    "audio2map.save('audio2map_full.h5') # This may not work well, but just in case we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Loss: \")\n",
    "print(history.history['loss'])\n",
    "print(\"Final Accuracy: \")\n",
    "print(history.history['accuracy'])\n",
    "print(\"Final Precision: \")\n",
    "print(history.history['precision'])\n",
    "print(\"Final Recall: \")\n",
    "print(history.history['recall'])\n",
    "print(\"Final F1: \")\n",
    "print(history.history['f1'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
