{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxtB_iV9kFTh"
   },
   "source": [
    "# Audio2Map\n",
    "This is an encoder-decoder model based off of seq2seq.\n",
    "\n",
    "It takes in an audio file for the music as an mp3 and outputs a fully functional map for the hit rhythm game Osu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9F8rmUKpkFTi"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from functools import reduce\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j7JIKMQkFTj",
    "outputId": "5c806a3f-183e-4ff3-9cc8-20675600da42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_features = 8\n",
    "STOP = torch.full((1, num_features), -1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPQSKcDkFTk"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here, we transform the input to a Constant-Q spectrogram spanning C1 to roughly C7. Then, for training, we obtain the pkl file containing the output vector representing the target output map.\n",
    "\n",
    "We also obtain the difficulty for our target output to feed into the decoder, when deployed, this will be input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MSXGo1phkFTl"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import traceback\n",
    "\n",
    "def convert_to_spectrogram(filename):\n",
    "    try:\n",
    "        targetSampleRate = 11025\n",
    "        y, sr = librosa.load(filename, sr=targetSampleRate)\n",
    "        C = np.abs(librosa.cqt(y, sr=targetSampleRate, n_bins=84, bins_per_octave=12))\n",
    "        S = librosa.amplitude_to_db(C, ref=np.max)\n",
    "        #plot the spectrogram\n",
    "\n",
    "        '''plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(S, sr=targetSampleRate, x_axis='time', y_axis='cqt_note')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Constant-Q power spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()'''\n",
    "        return S\n",
    "    except:\n",
    "        tsprint(\"ERROR: cannot convert \" + filename + \" to spectrogram.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def get_pkl(filename):\n",
    "    try:\n",
    "        return pickle.load(open(filename, 'rb'))\n",
    "    except:\n",
    "        tsprint(\"ERROR: .pkl file does not exist.\")\n",
    "        return -1\n",
    "\n",
    "def tsprint(s):\n",
    "    print(\"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \" + s)\n",
    "\n",
    "def parse_difficulty(filename):\n",
    "    if(not os.path.isfile(filename)):\n",
    "        tsprint(\"ERROR: map file does not exist. Removing.\")\n",
    "        os.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "        return -1\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            lines = f.readlines()\n",
    "        except:\n",
    "            tsprint(\"ERROR: cannot read lines of .osu file.\")\n",
    "\n",
    "\n",
    "    difficulty = [-1,-1,-1,-1,-1,-1]\n",
    "\n",
    "    for line in lines:\n",
    "        #difficulty\n",
    "        if line.startswith(\"HPDrainRate\"): difficulty[0] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"CircleSize\"): difficulty[1] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"OverallDifficulty\"): difficulty[2] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"ApproachRate\"): difficulty[3] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"SliderMultiplier\"): difficulty[4] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"SliderTickRate\"): difficulty[5] = float(line.split(\":\", 1)[1])\n",
    "        elif not (line.startswith(\"[Difficulty]\")): break\n",
    "\n",
    "    #check if all the difficulty stats are there\n",
    "    for val in difficulty:\n",
    "        if val == -1:\n",
    "            tsprint(\"ERROR: Not a valid osu! map due to insufficient stats. Removed file \" + filename + \".\")\n",
    "            os.remove(filename)\n",
    "            os.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "            return -1\n",
    "\n",
    "\n",
    "    return torch.tensor(difficulty)\n",
    "\n",
    "# TODO: Deprecate\n",
    "def load_data():\n",
    "    inputs = []\n",
    "    diffs = []\n",
    "    targets = []\n",
    "\n",
    "    curr_length = 0\n",
    "    counter = 0\n",
    "\n",
    "    if os.path.isfile(\"loaded_save.pkl\"):\n",
    "        inputs, diffs, targets = pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "        curr_length = len(inputs)\n",
    "\n",
    "\n",
    "    for pickle_root, pickle_dirs, pickle_files in os.walk(\"pickles\"):\n",
    "        for pickle_file in pickle_files:\n",
    "            counter += 1\n",
    "            if counter < curr_length: continue\n",
    "\n",
    "            tsprint(\"Parsing file \" + pickle_file)\n",
    "            inputs.append(convert_to_spectrogram(os.path.join(\"audio/\", pickle_file.split(\"_\")[0] + \".mp3\")))\n",
    "            diffs.append(parse_difficulty(\"maps/\" + pickle_file.split(\".\")[0] + \".osu\"))\n",
    "            targets.append(get_pkl(\"pickles/\" + pickle_file))\n",
    "\n",
    "            if counter % 10 == 0:\n",
    "                pickle.dump([inputs, diffs, targets], open(\"loaded_save.pkl\", 'wb'))\n",
    "                tsprint(\"Saved progress.\")\n",
    "                tsprint(\"Parsed \" + str(counter) + \" files.\")\n",
    "\n",
    "    return inputs, diffs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eyIAfiKKfizM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Audio2Map(Dataset):\n",
    "     def __init__(self, input_dir, maps_dir, target_dir):\n",
    "        self.in_dir = input_dir\n",
    "        self.maps_dir = maps_dir\n",
    "        self.tar_dir = target_dir\n",
    "        self.deleted_counter = 0\n",
    "     def __len__(self):\n",
    "        return len([name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))])\n",
    "     def __getitem__(self, idx):\n",
    "        # use the listdir() index 5Head\n",
    "        # Get the current map name w/o .osu\n",
    "        files = [name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))]\n",
    "        idx -= self.deleted_counter\n",
    "        idx = min(idx, len(files) - 1)\n",
    "        print(f\"files length: {len(files)}, index: {idx}\")\n",
    "        currfile = files[idx][:-4]\n",
    "        spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
    "        while(type(spec) == type(None)):\n",
    "            idx += 1\n",
    "            currfile = files[idx][:-4]\n",
    "            spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
    "            if isinstance(spec, int):\n",
    "                print(f'Could not get item at index {idx} due to parsing spectrogram.') \n",
    "                return -1\n",
    "        input = torch.tensor(spec.T).float()\n",
    "        diff = parse_difficulty(os.path.join(self.maps_dir, currfile + \".osu\"))\n",
    "        if isinstance(diff, int):\n",
    "            print(f'Could not get item at index {idx} due to parsing difficulty.') \n",
    "            return -1\n",
    "        diff = torch.t(parse_difficulty(os.path.join(self.maps_dir, currfile + \".osu\"))).float()\n",
    "        print(f\"filename: {currfile}\" )\n",
    "        #print(os.path.join(self.tar_dir, currfile + \".pkl\"))\n",
    "        #print(get_pkl(os.path.join(self.tar_dir, currfile + \".pkl\")))\n",
    "        out = get_pkl(os.path.join(self.tar_dir, currfile + \".pkl\"))[0].to_dense().float()\n",
    "        if isinstance(out, int):\n",
    "            print(f'Could not get item at index {idx} due to parsing pkl.') \n",
    "            return -1\n",
    "        #out = torch.cat((out, STOP), 0)\n",
    "        return input, diff, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "Oru0NiOFkFTm",
    "outputId": "f9097b70-497f-4ee7-db56-47eebcc03f2f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "# Now get the data >:D\n",
    "#inputs, diffs, targets = load_data() #pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "a2m_data = Audio2Map(\"audio/\", \"maps/\", \"pickles/\")\n",
    "\n",
    "test_split = 0.2\n",
    "train_data, test_data = random_split(a2m_data, [1-test_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NSjHbqOnBkXs"
   },
   "outputs": [],
   "source": [
    "for mp3file in os.listdir(\"audio/\"):\n",
    "    if not mp3file.split(\".\")[0] + \"_0.osu\" in os.listdir(\"maps/\"):\n",
    "        os.remove(\"audio/\" + mp3file)\n",
    "\n",
    "for mapfile in os.listdir(\"maps/\"):\n",
    "    if not mapfile.split(\"_\")[0] + \".mp3\" in os.listdir(\"audio/\"):\n",
    "        os.remove(\"maps/\" + mapfile)\n",
    "\n",
    "for pklfile in os.listdir(\"pickles/\"):\n",
    "    if not pklfile.split(\".\")[0] + \".osu\" in os.listdir(\"maps/\"):\n",
    "        os.remove(\"pickles/\" + pklfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xIdMBhp5kFTn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Must sample individually due to each input and output being different sizes :(\n",
    "train_dl = DataLoader(train_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "test_dl = DataLoader(test_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "# Output from dataloader is a list of size 3 containing a single input, difficulty, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RxIynZFfkFTo"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "- Given a song, we can generate a spectrogram\n",
    "- Take the spectrogram and produce a list of times (rythmic beats)\n",
    "\"\"\"\n",
    "# Encoder\n",
    "audio_dim = 84\n",
    "hidden_dim = 64\n",
    "\n",
    "# Use LSTM to predict the note timings of the song\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True, bidirectional=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CTIK8gcRkFTp"
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "from math import floor\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(num_features + 6, hidden_dim, num_layers=2, batch_first=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hiddenfc = nn.Linear(hidden_dim, hidden_dim//2, device=device)\n",
    "        self.outputfc = nn.Linear(hidden_dim//2, num_features, device=device)\n",
    "\n",
    "    def forward(self, encoder_out, encoder_hc, difficulty, target=None):\n",
    "        decoder_input = torch.zeros((1, num_features + 6), device=device)\n",
    "        decoder_hidden = encoder_hc\n",
    "        decoder_outputs = []\n",
    "\n",
    "        prev_percent = 0\n",
    "        #currStop = torch.cat((STOP.to(device), difficulty.unsqueeze(0)), 1)\n",
    "\n",
    "        #while(not torch.equal(decoder_input, currStop)):\n",
    "        iter = target.shape[0] if target != None else librosa.get_duration(S=encoder_out.T, sr=11025)*100\n",
    "        # (num samples/sr)*1000 = time in ms\n",
    "        for i in range(int(iter)):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_output = torch.round(decoder_output)\n",
    "            decoder_outputs.append(decoder_output.detach())\n",
    "\n",
    "            if target is not None:\n",
    "                curr_percent = floor(((i+1)/target.shape[0])*100)\n",
    "                if curr_percent > prev_percent:\n",
    "                    prev_percent = curr_percent\n",
    "                    #print(f\"Training...{curr_percent}%\")\n",
    "                decoder_input = torch.cat((target[i], difficulty), 0).unsqueeze(0)\n",
    "            else:\n",
    "                if (i+1) % 1000 == 0:\n",
    "                    print(f\"Timestep: {i+1}\")\n",
    "                decoder_input = torch.cat((decoder_output, difficulty.unsqueeze(0)), 1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, x, hc):\n",
    "        x, hc = self.lstm(x, hc)\n",
    "        drp = self.dropout(x)\n",
    "        hidden = self.hiddenfc(drp)\n",
    "        out = self.outputfc(hidden)\n",
    "        return out, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dzeN2TekFTp"
   },
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eFNJmcMfkFTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.293MB\n",
      "model size: 0.214MB\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "# Create models and offload to GPU for processing, if available\n",
    "enc = Encoder(0.4).to(device)\n",
    "dec = Decoder(0.4).to(device)\n",
    "\n",
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "\n",
    "print_model_size(enc)\n",
    "print_model_size(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IbZq22_XkFTq"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc):\n",
    "    # For now, data is a tuple of (x, diff, y)\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(data):\n",
    "        tsprint(f\"Current sample {i+1}\")\n",
    "        # Put data onto the GPU if available, otherwise its just on cpu unlucky\n",
    "        if isinstance(sample, int): \n",
    "            try:\n",
    "                data.deleted_counter += 1\n",
    "            except:\n",
    "                print(\"balls\")\n",
    "            continue\n",
    "        x = sample[0].to(device)\n",
    "        diff = sample[1].to(device)\n",
    "        y = sample[2].to(device)\n",
    "\n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hc = encoder(x)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hc, diff, target=y)\n",
    "\n",
    "        loss = lossfunc(decoder_outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(\"Sample \" + str(i + 1) + \" trained successfully!\")\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            torch.save(enc.state_dict(), 'encoder.pth')\n",
    "            torch.save(dec.state_dict(), 'decoder.pth')\n",
    "\n",
    "    return total_loss/len(data)\n",
    "\n",
    "def train(data, encoder, decoder, epochs=10, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    losshistory = []\n",
    "\n",
    "    lossfunc = nn.MSELoss()\n",
    "\n",
    "    enc_opt = Adam(enc.parameters(), lr=1e-4)\n",
    "    dec_opt = Adam(dec.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        tsprint(f\"Epoch: {epoch+1}\")\n",
    "        loss = train_epoch(data, encoder, decoder, enc_opt, dec_opt, lossfunc)\n",
    "        curr_time = time.time()\n",
    "        losshistory.append(loss)\n",
    "        print(f\"Loss: {loss} Time: {curr_time - start}\")\n",
    "    return losshistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ELEk-RkK0TfA"
   },
   "outputs": [],
   "source": [
    "def get_mem_req(enc, dec):\n",
    "    # register forward hooks to check intermediate activation size\n",
    "    acts = []\n",
    "    for name, module in enc.named_modules():\n",
    "        if name == 'classifier' or name == 'features':\n",
    "            continue\n",
    "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
    "    for name, module in dec.named_modules():\n",
    "        if name == 'classifier' or name == 'features':\n",
    "            continue\n",
    "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
    "\n",
    "    # execute single training step\n",
    "    X, diff, y_true = next(iter(train_dl))\n",
    "    # Forward pass\n",
    "    y_hat, hc = enc(X[:10000])\n",
    "    y_hat, _, _ = dec(y_hat, hc, diff, target=y_true[-10000:])\n",
    "    loss = nn.MSELoss()(y_hat, y_true[-10000:])\n",
    "    # Backward pass\n",
    "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
    "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
    "    enc_opt.zero_grad()\n",
    "    dec_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    enc_opt.step()\n",
    "    dec_opt.step()\n",
    "\n",
    "    # approximate memory requirements\n",
    "    model_param_size = sum([p.nelement() for p in enc.parameters()]) + sum([p.nelement() for p in dec.parameters()])\n",
    "    grad_size = model_param_size\n",
    "    batch_size = reduce((lambda x, y: x * y), X.shape)\n",
    "    optimizer_size = sum([p.nelement() for p in enc_opt.param_groups[0]['params']]) + sum([p.nelement() for p in dec_opt.param_groups[0]['params']])\n",
    "    act_size = sum([a.nelement() for a in acts])\n",
    "\n",
    "    total_nb_elements = model_param_size + grad_size + batch_size + optimizer_size + act_size\n",
    "    total_mb = total_nb_elements * 4 / 1024**2\n",
    "    print(total_mb)\n",
    "#get_mem_req(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1x1UlpN6kFT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-23 16:36:25] Epoch: 1\n",
      "files length: 916, index: 874\n",
      "filename: 647408_0\n",
      "[2024-06-23 16:36:27] Current sample 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitsu_/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_utils.py:315: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  result = torch.sparse_compressed_tensor(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m): enc\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m): dec\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, encoder, decoder, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     48\u001b[0m     tsprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     curr_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     51\u001b[0m     losshistory\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc)\u001b[0m\n\u001b[1;32m     22\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m decoder(encoder_outputs, encoder_hc, diff, target\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(decoder_outputs, y)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m encoder_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m decoder_opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('encoder.pth'): enc.load_state_dict(torch.load('encoder.pth'))\n",
    "if os.path.isfile('decoder.pth'): dec.load_state_dict(torch.load('decoder.pth'))\n",
    "\n",
    "train_loss = train(train_dl, enc, dec, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs3Ued2dvTbA"
   },
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), 'encoder.pth')\n",
    "torch.save(dec.state_dict(), 'decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s0vsJxykFT2"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_dFxINnkFT2"
   },
   "source": [
    "## Make some predictions :O\n",
    "\n",
    "Do some fine tuning for the model as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_k9kMdkFT2"
   },
   "outputs": [],
   "source": [
    "def decode_audio(encoder, decoder, audio, diff):\n",
    "    enc_out, states = encoder(audio)\n",
    "    out, _, _ = decoder(enc_out, states, diff)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc_k9kMdkFT2"
   },
   "outputs": [],
   "source": [
    "enc.load_state_dict(torch.load('encoder.pth'))\n",
    "dec.load_state_dict(torch.load('decoder.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3b-0iYjkFT2"
   },
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "num_samples = 1\n",
    "\n",
    "for i, sample in enumerate(test_dl):\n",
    "    x = sample[0].to(device)\n",
    "    diff = sample[1].to(device)\n",
    "    y = sample[2].to(device)\n",
    "    decoded_map = decode_audio(enc, dec, x, diff)[:y.shape[0], :]\n",
    "    print(\"Actual: \")\n",
    "    print(y.shape)\n",
    "    print(\"Predicted: \")\n",
    "    print(decoded_map.shape)\n",
    "\n",
    "    x = x.to(\"cpu\")\n",
    "    diff = diff.to(\"cpu\")\n",
    "    y = y.to(\"cpu\")\n",
    "    decoded_map = decoded_map.to(\"cpu\")\n",
    "\n",
    "    if not y.shape == decoded_map.shape:\n",
    "        print(\"Sizes are not the same. Disregarding.\")\n",
    "        continue\n",
    "    \n",
    "    curr_loss = nn.MSELoss()(y, decoded_map)\n",
    "    test_loss += curr_loss\n",
    "    num_samples += 1\n",
    "    \n",
    "test_loss /= num_samples\n",
    "print(f\"Average Testing Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEvlq_f-kFT2"
   },
   "source": [
    "## Evaluate that bish B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-zjvlU6kFT3"
   },
   "outputs": [],
   "source": [
    "full_enc = Encoder(0.4).to(device)\n",
    "full_dec = Decoder(0.4).to(device)\n",
    "a2m_dl = DataLoader(a2m_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "\n",
    "full_train_loss = train(a2m_dl, enc, dec, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c25IBt1rci8E"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(full_train_loss)), full_train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4CO4F-EcomA"
   },
   "outputs": [],
   "source": [
    "full_test_loss = 0\n",
    "num_samples = 1\n",
    "\n",
    "for i, sample in enumerate(test_dl):\n",
    "    x = sample[0].to(device)\n",
    "    diff = sample[1].to(device)\n",
    "    y = sample[2].to(device)\n",
    "    decoded_map = decode_audio(enc, dec, x, diff, y)[:y.shape[0]]\n",
    "    print(\"Actual: \")\n",
    "    print(y)\n",
    "    print(\"Predicted: \")\n",
    "    print(decoded_map)\n",
    "    curr_loss = nn.MSELoss()(y, decoded_map)\n",
    "    print(curr_loss)\n",
    "    full_test_loss += curr_loss\n",
    "full_test_loss /= num_samples\n",
    "print(f\"Average Testing Loss: {full_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "osu-beatmap-generator",
   "language": "python",
   "name": "osu-beatmap-generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
