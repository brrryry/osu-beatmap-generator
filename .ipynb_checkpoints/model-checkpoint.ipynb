{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxtB_iV9kFTh"
   },
   "source": [
    "# Audio2Map\n",
    "This is an encoder-decoder model based off of seq2seq.\n",
    "\n",
    "It takes in an audio file for the music as an mp3 and outputs a fully functional map for the hit rhythm game Osu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9F8rmUKpkFTi"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j7JIKMQkFTj",
    "outputId": "5c806a3f-183e-4ff3-9cc8-20675600da42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_features = 8\n",
    "STOP = torch.full((1, num_features), -1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPQSKcDkFTk"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here, we transform the input to a Constant-Q spectrogram spanning C1 to roughly C7. Then, for training, we obtain the pkl file containing the output vector representing the target output map.\n",
    "\n",
    "We also obtain the difficulty for our target output to feed into the decoder, when deployed, this will be input from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MSXGo1phkFTl"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import traceback\n",
    "\n",
    "def convert_to_spectrogram(filename):\n",
    "    try:\n",
    "        targetSampleRate = 11025\n",
    "        y, sr = librosa.load(filename, sr=targetSampleRate)\n",
    "        C = np.abs(librosa.cqt(y, sr=targetSampleRate, n_bins=84, bins_per_octave=12))\n",
    "        S = librosa.amplitude_to_db(C, ref=np.max)\n",
    "        #plot the spectrogram\n",
    "\n",
    "        '''plt.figure(figsize=(12, 4))\n",
    "        librosa.display.specshow(S, sr=targetSampleRate, x_axis='time', y_axis='cqt_note')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Constant-Q power spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()'''\n",
    "        return S\n",
    "    except:\n",
    "        tsprint(\"ERROR: cannot convert \" + filename + \" to spectrogram.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def get_pkl(filename):\n",
    "    try:\n",
    "        return pickle.load(open(filename, 'rb'))\n",
    "    except:\n",
    "        tsprint(\"ERROR: .pkl file does not exist.\")\n",
    "        return -1\n",
    "\n",
    "def tsprint(s):\n",
    "    print(\"[\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"] \" + s)\n",
    "\n",
    "def parse_difficulty(filename):\n",
    "    if(not os.path.isfile(filename)):\n",
    "        tsprint(\"ERROR: map file does not exist. Removing.\")\n",
    "        os.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "        return -1\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        try:\n",
    "            lines = f.readlines()\n",
    "        except:\n",
    "            tsprint(\"ERROR: cannot read lines of .osu file.\")\n",
    "\n",
    "\n",
    "    difficulty = [-1,-1,-1,-1,-1,-1]\n",
    "\n",
    "    for line in lines:\n",
    "        #difficulty\n",
    "        if line.startswith(\"HPDrainRate\"): difficulty[0] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"CircleSize\"): difficulty[1] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"OverallDifficulty\"): difficulty[2] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"ApproachRate\"): difficulty[3] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"SliderMultiplier\"): difficulty[4] = float(line.split(\":\", 1)[1])\n",
    "        elif line.startswith(\"SliderTickRate\"): difficulty[5] = float(line.split(\":\", 1)[1])\n",
    "        elif not (line.startswith(\"[Difficulty]\")): break\n",
    "\n",
    "    #check if all the difficulty stats are there\n",
    "    for val in difficulty:\n",
    "        if val == -1:\n",
    "            tsprint(\"ERROR: Not a valid osu! map due to insufficient stats. Removed file \" + filename + \".\")\n",
    "            os.remove(filename)\n",
    "            os.remove(\"pickles/\" + filename.split(\"/\")[1].split(\".\")[0] + \".pkl\")\n",
    "            return -1\n",
    "\n",
    "\n",
    "    return torch.tensor(difficulty)\n",
    "\n",
    "# TODO: Deprecate\n",
    "def load_data():\n",
    "    inputs = []\n",
    "    diffs = []\n",
    "    targets = []\n",
    "\n",
    "    curr_length = 0\n",
    "    counter = 0\n",
    "\n",
    "    if os.path.isfile(\"loaded_save.pkl\"):\n",
    "        inputs, diffs, targets = pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "        curr_length = len(inputs)\n",
    "\n",
    "\n",
    "    for pickle_root, pickle_dirs, pickle_files in os.walk(\"pickles\"):\n",
    "        for pickle_file in pickle_files:\n",
    "            counter += 1\n",
    "            if counter < curr_length: continue\n",
    "\n",
    "            tsprint(\"Parsing file \" + pickle_file)\n",
    "            inputs.append(convert_to_spectrogram(os.path.join(\"audio/\", pickle_file.split(\"_\")[0] + \".mp3\")))\n",
    "            diffs.append(parse_difficulty(\"maps/\" + pickle_file.split(\".\")[0] + \".osu\"))\n",
    "            targets.append(get_pkl(\"pickles/\" + pickle_file))\n",
    "\n",
    "            if counter % 10 == 0:\n",
    "                pickle.dump([inputs, diffs, targets], open(\"loaded_save.pkl\", 'wb'))\n",
    "                tsprint(\"Saved progress.\")\n",
    "                tsprint(\"Parsed \" + str(counter) + \" files.\")\n",
    "\n",
    "    return inputs, diffs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eyIAfiKKfizM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Audio2Map(Dataset):\n",
    "     def __init__(self, input_dir, maps_dir, target_dir):\n",
    "        self.in_dir = input_dir\n",
    "        self.maps_dir = maps_dir\n",
    "        self.tar_dir = target_dir\n",
    "        self.deleted_counter = 0\n",
    "     def __len__(self):\n",
    "        return len([name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))])\n",
    "     def __getitem__(self, idx):\n",
    "        # use the listdir() index 5Head\n",
    "        # Get the current map name w/o .osu\n",
    "        files = [name for name in os.listdir(self.tar_dir) if os.path.isfile(os.path.join(self.tar_dir, name))]\n",
    "        idx -= self.deleted_counter\n",
    "        idx = min(idx, len(files) - 1)\n",
    "        print(f\"files length: {len(files)}, index: {idx}\")\n",
    "        currfile = files[idx][:-4]\n",
    "        spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
    "        while(type(spec) == type(None)):\n",
    "            idx += 1\n",
    "            currfile = files[idx][:-4]\n",
    "            spec = convert_to_spectrogram(os.path.join(self.in_dir, currfile.split('_', 1)[0] + \".mp3\"))\n",
    "            if isinstance(spec, int):\n",
    "                print(f'Could not get item at index {idx} due to parsing spectrogram.') \n",
    "                return -1\n",
    "        input = torch.tensor(spec.T).float()\n",
    "        diff = parse_difficulty(os.path.join(self.maps_dir, currfile + \".osu\"))\n",
    "        if isinstance(diff, int):\n",
    "            print(f'Could not get item at index {idx} due to parsing difficulty.') \n",
    "            return -1\n",
    "        diff = torch.t(parse_difficulty(os.path.join(self.maps_dir, currfile + \".osu\"))).float()\n",
    "        print(f\"filename: {currfile}\" )\n",
    "        #print(os.path.join(self.tar_dir, currfile + \".pkl\"))\n",
    "        #print(get_pkl(os.path.join(self.tar_dir, currfile + \".pkl\")))\n",
    "        out = get_pkl(os.path.join(self.tar_dir, currfile + \".pkl\"))[0].to_dense().float()\n",
    "        if isinstance(out, int):\n",
    "            print(f'Could not get item at index {idx} due to parsing pkl.') \n",
    "            return -1\n",
    "        #out = torch.cat((out, STOP), 0)\n",
    "        return input, diff, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "Oru0NiOFkFTm",
    "outputId": "f9097b70-497f-4ee7-db56-47eebcc03f2f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "# Now get the data >:D\n",
    "#inputs, diffs, targets = load_data() #pickle.load(open(\"loaded_save.pkl\", 'rb'))\n",
    "a2m_data = Audio2Map(\"audio/\", \"maps/\", \"pickles/\")\n",
    "\n",
    "test_split = 0.2\n",
    "train_data, test_data = random_split(a2m_data, [1-test_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NSjHbqOnBkXs"
   },
   "outputs": [],
   "source": [
    "for mp3file in os.listdir(\"audio/\"):\n",
    "    if not mp3file.split(\".\")[0] + \"_0.osu\" in os.listdir(\"maps/\"):\n",
    "        os.remove(\"audio/\" + mp3file)\n",
    "\n",
    "for mapfile in os.listdir(\"maps/\"):\n",
    "    if not mapfile.split(\"_\")[0] + \".mp3\" in os.listdir(\"audio/\"):\n",
    "        os.remove(\"maps/\" + mapfile)\n",
    "\n",
    "for pklfile in os.listdir(\"pickles/\"):\n",
    "    if not pklfile.split(\".\")[0] + \".osu\" in os.listdir(\"maps/\"):\n",
    "        os.remove(\"pickles/\" + pklfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xIdMBhp5kFTn"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Must sample individually due to each input and output being different sizes :(\n",
    "train_dl = DataLoader(train_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "test_dl = DataLoader(test_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "# Output from dataloader is a list of size 3 containing a single input, difficulty, and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RxIynZFfkFTo"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "- Given a song, we can generate a spectrogram\n",
    "- Take the spectrogram and produce a list of times (rythmic beats)\n",
    "\"\"\"\n",
    "# Encoder\n",
    "audio_dim = 84\n",
    "hidden_dim = 64\n",
    "\n",
    "# Use LSTM to predict the note timings of the song\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(audio_dim, hidden_dim, batch_first=True, bidirectional=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        out = self.dropout(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CTIK8gcRkFTp"
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "from math import floor\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(num_features + 6, hidden_dim, num_layers=2, batch_first=True, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hiddenfc = nn.Linear(hidden_dim, hidden_dim//2, device=device)\n",
    "        self.outputfc = nn.Linear(hidden_dim//2, num_features, device=device)\n",
    "\n",
    "    def forward(self, encoder_out, encoder_hc, difficulty, target=None):\n",
    "        decoder_input = torch.zeros((1, num_features + 6), device=device)\n",
    "        decoder_hidden = encoder_hc\n",
    "        decoder_outputs = []\n",
    "\n",
    "        prev_percent = 0\n",
    "        #currStop = torch.cat((STOP.to(device), difficulty.unsqueeze(0)), 1)\n",
    "\n",
    "        #while(not torch.equal(decoder_input, currStop)):\n",
    "        iter = target.shape[0] if target != None else librosa.get_duration(S=encoder_out.T, sr=11025)*100\n",
    "        # (num samples/sr)*1000 = time in ms\n",
    "        print(f'Requires {int(iter)} iterations')\n",
    "        for i in range(int(iter)):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target is not None:\n",
    "                curr_percent = floor(((i+1)/target.shape[0])*100)\n",
    "                if curr_percent > prev_percent:\n",
    "                    prev_percent = curr_percent\n",
    "                    #print(f\"Training...{curr_percent}%\")\n",
    "                decoder_input = torch.cat((target[i], difficulty), 0).unsqueeze(0)\n",
    "            else:\n",
    "                if (i+1) % 1000 == 0:\n",
    "                    print(f\"Timestep: {i+1}\")\n",
    "                decoder_input = torch.cat((decoder_output, difficulty.unsqueeze(0)), 1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, x, hc):\n",
    "        x, hc = self.lstm(x, hc)\n",
    "        drp = self.dropout(x)\n",
    "        hidden = self.hiddenfc(drp)\n",
    "        out = self.outputfc(hidden)\n",
    "        return out, hc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dzeN2TekFTp"
   },
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eFNJmcMfkFTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.293MB\n",
      "model size: 0.214MB\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "# Create models and offload to GPU for processing, if available\n",
    "enc = Encoder(0.4).to(device)\n",
    "dec = Decoder(0.4).to(device)\n",
    "\n",
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "\n",
    "print_model_size(enc)\n",
    "print_model_size(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IbZq22_XkFTq"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train_epoch(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc):\n",
    "    # For now, data is a tuple of (x, diff, y)\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(data):\n",
    "        tsprint(f\"Current sample {i+1}\")\n",
    "        # Put data onto the GPU if available, otherwise its just on cpu unlucky\n",
    "        if isinstance(sample, int): \n",
    "            try:\n",
    "                data.deleted_counter += 1\n",
    "            except:\n",
    "                print(\"balls\")\n",
    "            continue\n",
    "        x = sample[0].to(device)\n",
    "        diff = sample[1].to(device)\n",
    "        y = sample[2].to(device)\n",
    "\n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hc = encoder(x)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hc, diff, target=y)\n",
    "\n",
    "        loss = lossfunc(decoder_outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(\"Sample \" + str(i + 1) + \" trained successfully!\")\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            torch.save(enc.state_dict(), 'encoder.pth')\n",
    "            torch.save(dec.state_dict(), 'decoder.pth')\n",
    "\n",
    "    return total_loss/len(data)\n",
    "\n",
    "def train(data, encoder, decoder, epochs=10, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    losshistory = []\n",
    "\n",
    "    lossfunc = nn.MSELoss()\n",
    "\n",
    "    enc_opt = Adam(enc.parameters(), lr=1e-4)\n",
    "    dec_opt = Adam(dec.parameters(), lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        tsprint(f\"Epoch: {epoch+1}\")\n",
    "        loss = train_epoch(data, encoder, decoder, enc_opt, dec_opt, lossfunc)\n",
    "        curr_time = time.time()\n",
    "        losshistory.append(loss)\n",
    "        print(f\"Loss: {loss} Time: {curr_time - start}\")\n",
    "    return losshistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ELEk-RkK0TfA"
   },
   "outputs": [],
   "source": [
    "def get_mem_req(enc, dec):\n",
    "    # register forward hooks to check intermediate activation size\n",
    "    acts = []\n",
    "    for name, module in enc.named_modules():\n",
    "        if name == 'classifier' or name == 'features':\n",
    "            continue\n",
    "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
    "    for name, module in dec.named_modules():\n",
    "        if name == 'classifier' or name == 'features':\n",
    "            continue\n",
    "        module.register_forward_hook(lambda m, input, output: acts.append(output[0].detach()))\n",
    "\n",
    "    # execute single training step\n",
    "    X, diff, y_true = next(iter(train_dl))\n",
    "    # Forward pass\n",
    "    y_hat, hc = enc(X[:10000])\n",
    "    y_hat, _, _ = dec(y_hat, hc, diff, target=y_true[-10000:])\n",
    "    loss = nn.MSELoss()(y_hat, y_true[-10000:])\n",
    "    # Backward pass\n",
    "    enc_opt = Adam(enc.parameters(), lr=0.01)\n",
    "    dec_opt = Adam(dec.parameters(), lr=0.01)\n",
    "    enc_opt.zero_grad()\n",
    "    dec_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    enc_opt.step()\n",
    "    dec_opt.step()\n",
    "\n",
    "    # approximate memory requirements\n",
    "    model_param_size = sum([p.nelement() for p in enc.parameters()]) + sum([p.nelement() for p in dec.parameters()])\n",
    "    grad_size = model_param_size\n",
    "    batch_size = reduce((lambda x, y: x * y), X.shape)\n",
    "    optimizer_size = sum([p.nelement() for p in enc_opt.param_groups[0]['params']]) + sum([p.nelement() for p in dec_opt.param_groups[0]['params']])\n",
    "    act_size = sum([a.nelement() for a in acts])\n",
    "\n",
    "    total_nb_elements = model_param_size + grad_size + batch_size + optimizer_size + act_size\n",
    "    total_mb = total_nb_elements * 4 / 1024**2\n",
    "    print(total_mb)\n",
    "#get_mem_req(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1x1UlpN6kFT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-23 00:02:43] Epoch: 1\n",
      "files length: 943, index: 812\n",
      "filename: 616966_3\n",
      "[2024-06-23 00:02:44] Current sample 1\n",
      "Requires 11419 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitsu_/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_utils.py:315: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  result = torch.sparse_compressed_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 trained successfully!\n",
      "files length: 943, index: 144\n",
      "filename: 671013_0\n",
      "[2024-06-23 00:02:50] Current sample 2\n",
      "Requires 6668 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:III_get_side_info():202] error: big_values too large!\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1801] error: dequantization failed!\n",
      "Note: Illegal Audio-MPEG-Header 0x49d91740 at offset 1245594.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 329 bytes in input.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2 trained successfully!\n",
      "files length: 943, index: 811\n",
      "filename: 510668_0\n",
      "[2024-06-23 00:02:55] Current sample 3\n",
      "Requires 1172 iterations\n",
      "Sample 3 trained successfully!\n",
      "files length: 943, index: 113\n",
      "filename: 785276_1\n",
      "[2024-06-23 00:02:56] Current sample 4\n",
      "Requires 8582 iterations\n",
      "Sample 4 trained successfully!\n",
      "files length: 943, index: 929\n",
      "filename: 839746_0\n",
      "[2024-06-23 00:03:01] Current sample 5\n",
      "Requires 21791 iterations\n",
      "Sample 5 trained successfully!\n",
      "files length: 943, index: 353\n",
      "filename: 616966_2\n",
      "[2024-06-23 00:03:13] Current sample 6\n",
      "Requires 11419 iterations\n",
      "Sample 6 trained successfully!\n",
      "files length: 943, index: 173\n",
      "filename: 780062_0\n",
      "[2024-06-23 00:03:20] Current sample 7\n",
      "Requires 21217 iterations\n",
      "Sample 7 trained successfully!\n",
      "files length: 943, index: 478\n",
      "filename: 120646_0\n",
      "[2024-06-23 00:03:32] Current sample 8\n",
      "Requires 8263 iterations\n",
      "Sample 8 trained successfully!\n",
      "files length: 943, index: 523\n",
      "filename: 981885_0\n",
      "[2024-06-23 00:03:38] Current sample 9\n",
      "Requires 10773 iterations\n",
      "Sample 9 trained successfully!\n",
      "files length: 943, index: 260\n",
      "filename: 504091_0\n",
      "[2024-06-23 00:03:44] Current sample 10\n",
      "Requires 13279 iterations\n",
      "Sample 10 trained successfully!\n",
      "files length: 943, index: 926\n",
      "filename: 303319_2\n",
      "[2024-06-23 00:03:51] Current sample 11\n",
      "Requires 8941 iterations\n",
      "Sample 11 trained successfully!\n",
      "files length: 943, index: 188\n",
      "filename: 870299_0\n",
      "[2024-06-23 00:03:57] Current sample 12\n",
      "Requires 19780 iterations\n",
      "Sample 12 trained successfully!\n",
      "files length: 943, index: 825\n",
      "filename: 666449_0\n",
      "[2024-06-23 00:04:09] Current sample 13\n",
      "Requires 3325 iterations\n",
      "Sample 13 trained successfully!\n",
      "files length: 943, index: 70\n",
      "filename: 608046_0\n",
      "[2024-06-23 00:04:11] Current sample 14\n",
      "Requires 17675 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m): enc\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m): dec\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, encoder, decoder, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     44\u001b[0m     tsprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     curr_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     47\u001b[0m     losshistory\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc)\u001b[0m\n\u001b[1;32m     22\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m decoder(encoder_outputs, encoder_hc, diff, target\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(decoder_outputs, y)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m encoder_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m decoder_opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.isfile('encoder.pth'): enc.load_state_dict(torch.load('encoder.pth'))\n",
    "if os.path.isfile('decoder.pth'): dec.load_state_dict(torch.load('decoder.pth'))\n",
    "\n",
    "train_loss = train(train_dl, enc, dec, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Xs3Ued2dvTbA"
   },
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), 'encoder.pth')\n",
    "torch.save(dec.state_dict(), 'decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-s0vsJxykFT2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_dFxINnkFT2"
   },
   "source": [
    "## Make some predictions :O\n",
    "\n",
    "Do some fine tuning for the model as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Fc_k9kMdkFT2"
   },
   "outputs": [],
   "source": [
    "def decode_audio(encoder, decoder, audio, diff):\n",
    "    print(\"b\")\n",
    "    enc_out, states = encoder(audio)\n",
    "    out, _, _ = decoder(enc_out, states, diff)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fc_k9kMdkFT2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.load_state_dict(torch.load('encoder.pth'))\n",
    "dec.load_state_dict(torch.load('decoder.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W3b-0iYjkFT2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files length: 40, index: 8\n",
      "filename: 387254_0\n",
      "b\n",
      "Requires 18404 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kitsu_/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_utils.py:315: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  result = torch.sparse_compressed_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Actual: \n",
      "torch.Size([18104, 8])\n",
      "Predicted: \n",
      "torch.Size([18104, 8])\n",
      "files length: 40, index: 5\n",
      "filename: 7484_1\n",
      "b\n",
      "Requires 21575 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Timestep: 18500\n",
      "Timestep: 18600\n",
      "Timestep: 18700\n",
      "Timestep: 18800\n",
      "Timestep: 18900\n",
      "Timestep: 19000\n",
      "Timestep: 19100\n",
      "Timestep: 19200\n",
      "Timestep: 19300\n",
      "Timestep: 19400\n",
      "Timestep: 19500\n",
      "Timestep: 19600\n",
      "Timestep: 19700\n",
      "Timestep: 19800\n",
      "Timestep: 19900\n",
      "Timestep: 20000\n",
      "Timestep: 20100\n",
      "Timestep: 20200\n",
      "Timestep: 20300\n",
      "Timestep: 20400\n",
      "Timestep: 20500\n",
      "Timestep: 20600\n",
      "Timestep: 20700\n",
      "Timestep: 20800\n",
      "Timestep: 20900\n",
      "Timestep: 21000\n",
      "Timestep: 21100\n",
      "Timestep: 21200\n",
      "Timestep: 21300\n",
      "Timestep: 21400\n",
      "Timestep: 21500\n",
      "Actual: \n",
      "torch.Size([18957, 8])\n",
      "Predicted: \n",
      "torch.Size([18957, 8])\n",
      "files length: 40, index: 4\n",
      "filename: 540207_0\n",
      "b\n",
      "Requires 10021 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Actual: \n",
      "torch.Size([9294, 8])\n",
      "Predicted: \n",
      "torch.Size([9294, 8])\n",
      "files length: 40, index: 18\n",
      "filename: 697750_3\n",
      "b\n",
      "Requires 23610 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Timestep: 18500\n",
      "Timestep: 18600\n",
      "Timestep: 18700\n",
      "Timestep: 18800\n",
      "Timestep: 18900\n",
      "Timestep: 19000\n",
      "Timestep: 19100\n",
      "Timestep: 19200\n",
      "Timestep: 19300\n",
      "Timestep: 19400\n",
      "Timestep: 19500\n",
      "Timestep: 19600\n",
      "Timestep: 19700\n",
      "Timestep: 19800\n",
      "Timestep: 19900\n",
      "Timestep: 20000\n",
      "Timestep: 20100\n",
      "Timestep: 20200\n",
      "Timestep: 20300\n",
      "Timestep: 20400\n",
      "Timestep: 20500\n",
      "Timestep: 20600\n",
      "Timestep: 20700\n",
      "Timestep: 20800\n",
      "Timestep: 20900\n",
      "Timestep: 21000\n",
      "Timestep: 21100\n",
      "Timestep: 21200\n",
      "Timestep: 21300\n",
      "Timestep: 21400\n",
      "Timestep: 21500\n",
      "Timestep: 21600\n",
      "Timestep: 21700\n",
      "Timestep: 21800\n",
      "Timestep: 21900\n",
      "Timestep: 22000\n",
      "Timestep: 22100\n",
      "Timestep: 22200\n",
      "Timestep: 22300\n",
      "Timestep: 22400\n",
      "Timestep: 22500\n",
      "Timestep: 22600\n",
      "Timestep: 22700\n",
      "Timestep: 22800\n",
      "Timestep: 22900\n",
      "Timestep: 23000\n",
      "Timestep: 23100\n",
      "Timestep: 23200\n",
      "Timestep: 23300\n",
      "Timestep: 23400\n",
      "Timestep: 23500\n",
      "Timestep: 23600\n",
      "Actual: \n",
      "torch.Size([23065, 8])\n",
      "Predicted: \n",
      "torch.Size([23065, 8])\n",
      "files length: 40, index: 30\n",
      "filename: 697750_0\n",
      "b\n",
      "Requires 23610 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Timestep: 18500\n",
      "Timestep: 18600\n",
      "Timestep: 18700\n",
      "Timestep: 18800\n",
      "Timestep: 18900\n",
      "Timestep: 19000\n",
      "Timestep: 19100\n",
      "Timestep: 19200\n",
      "Timestep: 19300\n",
      "Timestep: 19400\n",
      "Timestep: 19500\n",
      "Timestep: 19600\n",
      "Timestep: 19700\n",
      "Timestep: 19800\n",
      "Timestep: 19900\n",
      "Timestep: 20000\n",
      "Timestep: 20100\n",
      "Timestep: 20200\n",
      "Timestep: 20300\n",
      "Timestep: 20400\n",
      "Timestep: 20500\n",
      "Timestep: 20600\n",
      "Timestep: 20700\n",
      "Timestep: 20800\n",
      "Timestep: 20900\n",
      "Timestep: 21000\n",
      "Timestep: 21100\n",
      "Timestep: 21200\n",
      "Timestep: 21300\n",
      "Timestep: 21400\n",
      "Timestep: 21500\n",
      "Timestep: 21600\n",
      "Timestep: 21700\n",
      "Timestep: 21800\n",
      "Timestep: 21900\n",
      "Timestep: 22000\n",
      "Timestep: 22100\n",
      "Timestep: 22200\n",
      "Timestep: 22300\n",
      "Timestep: 22400\n",
      "Timestep: 22500\n",
      "Timestep: 22600\n",
      "Timestep: 22700\n",
      "Timestep: 22800\n",
      "Timestep: 22900\n",
      "Timestep: 23000\n",
      "Timestep: 23100\n",
      "Timestep: 23200\n",
      "Timestep: 23300\n",
      "Timestep: 23400\n",
      "Timestep: 23500\n",
      "Timestep: 23600\n",
      "Actual: \n",
      "torch.Size([23065, 8])\n",
      "Predicted: \n",
      "torch.Size([23065, 8])\n",
      "files length: 40, index: 27\n",
      "filename: 867457_1\n",
      "b\n",
      "Requires 47405 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Timestep: 18500\n",
      "Timestep: 18600\n",
      "Timestep: 18700\n",
      "Timestep: 18800\n",
      "Timestep: 18900\n",
      "Timestep: 19000\n",
      "Timestep: 19100\n",
      "Timestep: 19200\n",
      "Timestep: 19300\n",
      "Timestep: 19400\n",
      "Timestep: 19500\n",
      "Timestep: 19600\n",
      "Timestep: 19700\n",
      "Timestep: 19800\n",
      "Timestep: 19900\n",
      "Timestep: 20000\n",
      "Timestep: 20100\n",
      "Timestep: 20200\n",
      "Timestep: 20300\n",
      "Timestep: 20400\n",
      "Timestep: 20500\n",
      "Timestep: 20600\n",
      "Timestep: 20700\n",
      "Timestep: 20800\n",
      "Timestep: 20900\n",
      "Timestep: 21000\n",
      "Timestep: 21100\n",
      "Timestep: 21200\n",
      "Timestep: 21300\n",
      "Timestep: 21400\n",
      "Timestep: 21500\n",
      "Timestep: 21600\n",
      "Timestep: 21700\n",
      "Timestep: 21800\n",
      "Timestep: 21900\n",
      "Timestep: 22000\n",
      "Timestep: 22100\n",
      "Timestep: 22200\n",
      "Timestep: 22300\n",
      "Timestep: 22400\n",
      "Timestep: 22500\n",
      "Timestep: 22600\n",
      "Timestep: 22700\n",
      "Timestep: 22800\n",
      "Timestep: 22900\n",
      "Timestep: 23000\n",
      "Timestep: 23100\n",
      "Timestep: 23200\n",
      "Timestep: 23300\n",
      "Timestep: 23400\n",
      "Timestep: 23500\n",
      "Timestep: 23600\n",
      "Timestep: 23700\n",
      "Timestep: 23800\n",
      "Timestep: 23900\n",
      "Timestep: 24000\n",
      "Timestep: 24100\n",
      "Timestep: 24200\n",
      "Timestep: 24300\n",
      "Timestep: 24400\n",
      "Timestep: 24500\n",
      "Timestep: 24600\n",
      "Timestep: 24700\n",
      "Timestep: 24800\n",
      "Timestep: 24900\n",
      "Timestep: 25000\n",
      "Timestep: 25100\n",
      "Timestep: 25200\n",
      "Timestep: 25300\n",
      "Timestep: 25400\n",
      "Timestep: 25500\n",
      "Timestep: 25600\n",
      "Timestep: 25700\n",
      "Timestep: 25800\n",
      "Timestep: 25900\n",
      "Timestep: 26000\n",
      "Timestep: 26100\n",
      "Timestep: 26200\n",
      "Timestep: 26300\n",
      "Timestep: 26400\n",
      "Timestep: 26500\n",
      "Timestep: 26600\n",
      "Timestep: 26700\n",
      "Timestep: 26800\n",
      "Timestep: 26900\n",
      "Timestep: 27000\n",
      "Timestep: 27100\n",
      "Timestep: 27200\n",
      "Timestep: 27300\n",
      "Timestep: 27400\n",
      "Timestep: 27500\n",
      "Timestep: 27600\n",
      "Timestep: 27700\n",
      "Timestep: 27800\n",
      "Timestep: 27900\n",
      "Timestep: 28000\n",
      "Timestep: 28100\n",
      "Timestep: 28200\n",
      "Timestep: 28300\n",
      "Timestep: 28400\n",
      "Timestep: 28500\n",
      "Timestep: 28600\n",
      "Timestep: 28700\n",
      "Timestep: 28800\n",
      "Timestep: 28900\n",
      "Timestep: 29000\n",
      "Timestep: 29100\n",
      "Timestep: 29200\n",
      "Timestep: 29300\n",
      "Timestep: 29400\n",
      "Timestep: 29500\n",
      "Timestep: 29600\n",
      "Timestep: 29700\n",
      "Timestep: 29800\n",
      "Timestep: 29900\n",
      "Timestep: 30000\n",
      "Timestep: 30100\n",
      "Timestep: 30200\n",
      "Timestep: 30300\n",
      "Timestep: 30400\n",
      "Timestep: 30500\n",
      "Timestep: 30600\n",
      "Timestep: 30700\n",
      "Timestep: 30800\n",
      "Timestep: 30900\n",
      "Timestep: 31000\n",
      "Timestep: 31100\n",
      "Timestep: 31200\n",
      "Timestep: 31300\n",
      "Timestep: 31400\n",
      "Timestep: 31500\n",
      "Timestep: 31600\n",
      "Timestep: 31700\n",
      "Timestep: 31800\n",
      "Timestep: 31900\n",
      "Timestep: 32000\n",
      "Timestep: 32100\n",
      "Timestep: 32200\n",
      "Timestep: 32300\n",
      "Timestep: 32400\n",
      "Timestep: 32500\n",
      "Timestep: 32600\n",
      "Timestep: 32700\n",
      "Timestep: 32800\n",
      "Timestep: 32900\n",
      "Timestep: 33000\n",
      "Timestep: 33100\n",
      "Timestep: 33200\n",
      "Timestep: 33300\n",
      "Timestep: 33400\n",
      "Timestep: 33500\n",
      "Timestep: 33600\n",
      "Timestep: 33700\n",
      "Timestep: 33800\n",
      "Timestep: 33900\n",
      "Timestep: 34000\n",
      "Timestep: 34100\n",
      "Timestep: 34200\n",
      "Timestep: 34300\n",
      "Timestep: 34400\n",
      "Timestep: 34500\n",
      "Timestep: 34600\n",
      "Timestep: 34700\n",
      "Timestep: 34800\n",
      "Timestep: 34900\n",
      "Timestep: 35000\n",
      "Timestep: 35100\n",
      "Timestep: 35200\n",
      "Timestep: 35300\n",
      "Timestep: 35400\n",
      "Timestep: 35500\n",
      "Timestep: 35600\n",
      "Timestep: 35700\n",
      "Timestep: 35800\n",
      "Timestep: 35900\n",
      "Timestep: 36000\n",
      "Timestep: 36100\n",
      "Timestep: 36200\n",
      "Timestep: 36300\n",
      "Timestep: 36400\n",
      "Timestep: 36500\n",
      "Timestep: 36600\n",
      "Timestep: 36700\n",
      "Timestep: 36800\n",
      "Timestep: 36900\n",
      "Timestep: 37000\n",
      "Timestep: 37100\n",
      "Timestep: 37200\n",
      "Timestep: 37300\n",
      "Timestep: 37400\n",
      "Timestep: 37500\n",
      "Timestep: 37600\n",
      "Timestep: 37700\n",
      "Timestep: 37800\n",
      "Timestep: 37900\n",
      "Timestep: 38000\n",
      "Timestep: 38100\n",
      "Timestep: 38200\n",
      "Timestep: 38300\n",
      "Timestep: 38400\n",
      "Timestep: 38500\n",
      "Timestep: 38600\n",
      "Timestep: 38700\n",
      "Timestep: 38800\n",
      "Timestep: 38900\n",
      "Timestep: 39000\n",
      "Timestep: 39100\n",
      "Timestep: 39200\n",
      "Timestep: 39300\n",
      "Timestep: 39400\n",
      "Timestep: 39500\n",
      "Timestep: 39600\n",
      "Timestep: 39700\n",
      "Timestep: 39800\n",
      "Timestep: 39900\n",
      "Timestep: 40000\n",
      "Timestep: 40100\n",
      "Timestep: 40200\n",
      "Timestep: 40300\n",
      "Timestep: 40400\n",
      "Timestep: 40500\n",
      "Timestep: 40600\n",
      "Timestep: 40700\n",
      "Timestep: 40800\n",
      "Timestep: 40900\n",
      "Timestep: 41000\n",
      "Timestep: 41100\n",
      "Timestep: 41200\n",
      "Timestep: 41300\n",
      "Timestep: 41400\n",
      "Timestep: 41500\n",
      "Timestep: 41600\n",
      "Timestep: 41700\n",
      "Timestep: 41800\n",
      "Timestep: 41900\n",
      "Timestep: 42000\n",
      "Timestep: 42100\n",
      "Timestep: 42200\n",
      "Timestep: 42300\n",
      "Timestep: 42400\n",
      "Timestep: 42500\n",
      "Timestep: 42600\n",
      "Timestep: 42700\n",
      "Timestep: 42800\n",
      "Timestep: 42900\n",
      "Timestep: 43000\n",
      "Timestep: 43100\n",
      "Timestep: 43200\n",
      "Timestep: 43300\n",
      "Timestep: 43400\n",
      "Timestep: 43500\n",
      "Timestep: 43600\n",
      "Timestep: 43700\n",
      "Timestep: 43800\n",
      "Timestep: 43900\n",
      "Timestep: 44000\n",
      "Timestep: 44100\n",
      "Timestep: 44200\n",
      "Timestep: 44300\n",
      "Timestep: 44400\n",
      "Timestep: 44500\n",
      "Timestep: 44600\n",
      "Timestep: 44700\n",
      "Timestep: 44800\n",
      "Timestep: 44900\n",
      "Timestep: 45000\n",
      "Timestep: 45100\n",
      "Timestep: 45200\n",
      "Timestep: 45300\n",
      "Timestep: 45400\n",
      "Timestep: 45500\n",
      "Timestep: 45600\n",
      "Timestep: 45700\n",
      "Timestep: 45800\n",
      "Timestep: 45900\n",
      "Timestep: 46000\n",
      "Timestep: 46100\n",
      "Timestep: 46200\n",
      "Timestep: 46300\n",
      "Timestep: 46400\n",
      "Timestep: 46500\n",
      "Timestep: 46600\n",
      "Timestep: 46700\n",
      "Timestep: 46800\n",
      "Timestep: 46900\n",
      "Timestep: 47000\n",
      "Timestep: 47100\n",
      "Timestep: 47200\n",
      "Timestep: 47300\n",
      "Timestep: 47400\n",
      "Actual: \n",
      "torch.Size([46366, 8])\n",
      "Predicted: \n",
      "torch.Size([46366, 8])\n",
      "files length: 40, index: 22\n",
      "filename: 89744_1\n",
      "b\n",
      "Requires 14489 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Actual: \n",
      "torch.Size([14109, 8])\n",
      "Predicted: \n",
      "torch.Size([14109, 8])\n",
      "files length: 40, index: 15\n",
      "filename: 884845_0\n",
      "b\n",
      "Requires 27116 iterations\n",
      "Timestep: 100\n",
      "Timestep: 200\n",
      "Timestep: 300\n",
      "Timestep: 400\n",
      "Timestep: 500\n",
      "Timestep: 600\n",
      "Timestep: 700\n",
      "Timestep: 800\n",
      "Timestep: 900\n",
      "Timestep: 1000\n",
      "Timestep: 1100\n",
      "Timestep: 1200\n",
      "Timestep: 1300\n",
      "Timestep: 1400\n",
      "Timestep: 1500\n",
      "Timestep: 1600\n",
      "Timestep: 1700\n",
      "Timestep: 1800\n",
      "Timestep: 1900\n",
      "Timestep: 2000\n",
      "Timestep: 2100\n",
      "Timestep: 2200\n",
      "Timestep: 2300\n",
      "Timestep: 2400\n",
      "Timestep: 2500\n",
      "Timestep: 2600\n",
      "Timestep: 2700\n",
      "Timestep: 2800\n",
      "Timestep: 2900\n",
      "Timestep: 3000\n",
      "Timestep: 3100\n",
      "Timestep: 3200\n",
      "Timestep: 3300\n",
      "Timestep: 3400\n",
      "Timestep: 3500\n",
      "Timestep: 3600\n",
      "Timestep: 3700\n",
      "Timestep: 3800\n",
      "Timestep: 3900\n",
      "Timestep: 4000\n",
      "Timestep: 4100\n",
      "Timestep: 4200\n",
      "Timestep: 4300\n",
      "Timestep: 4400\n",
      "Timestep: 4500\n",
      "Timestep: 4600\n",
      "Timestep: 4700\n",
      "Timestep: 4800\n",
      "Timestep: 4900\n",
      "Timestep: 5000\n",
      "Timestep: 5100\n",
      "Timestep: 5200\n",
      "Timestep: 5300\n",
      "Timestep: 5400\n",
      "Timestep: 5500\n",
      "Timestep: 5600\n",
      "Timestep: 5700\n",
      "Timestep: 5800\n",
      "Timestep: 5900\n",
      "Timestep: 6000\n",
      "Timestep: 6100\n",
      "Timestep: 6200\n",
      "Timestep: 6300\n",
      "Timestep: 6400\n",
      "Timestep: 6500\n",
      "Timestep: 6600\n",
      "Timestep: 6700\n",
      "Timestep: 6800\n",
      "Timestep: 6900\n",
      "Timestep: 7000\n",
      "Timestep: 7100\n",
      "Timestep: 7200\n",
      "Timestep: 7300\n",
      "Timestep: 7400\n",
      "Timestep: 7500\n",
      "Timestep: 7600\n",
      "Timestep: 7700\n",
      "Timestep: 7800\n",
      "Timestep: 7900\n",
      "Timestep: 8000\n",
      "Timestep: 8100\n",
      "Timestep: 8200\n",
      "Timestep: 8300\n",
      "Timestep: 8400\n",
      "Timestep: 8500\n",
      "Timestep: 8600\n",
      "Timestep: 8700\n",
      "Timestep: 8800\n",
      "Timestep: 8900\n",
      "Timestep: 9000\n",
      "Timestep: 9100\n",
      "Timestep: 9200\n",
      "Timestep: 9300\n",
      "Timestep: 9400\n",
      "Timestep: 9500\n",
      "Timestep: 9600\n",
      "Timestep: 9700\n",
      "Timestep: 9800\n",
      "Timestep: 9900\n",
      "Timestep: 10000\n",
      "Timestep: 10100\n",
      "Timestep: 10200\n",
      "Timestep: 10300\n",
      "Timestep: 10400\n",
      "Timestep: 10500\n",
      "Timestep: 10600\n",
      "Timestep: 10700\n",
      "Timestep: 10800\n",
      "Timestep: 10900\n",
      "Timestep: 11000\n",
      "Timestep: 11100\n",
      "Timestep: 11200\n",
      "Timestep: 11300\n",
      "Timestep: 11400\n",
      "Timestep: 11500\n",
      "Timestep: 11600\n",
      "Timestep: 11700\n",
      "Timestep: 11800\n",
      "Timestep: 11900\n",
      "Timestep: 12000\n",
      "Timestep: 12100\n",
      "Timestep: 12200\n",
      "Timestep: 12300\n",
      "Timestep: 12400\n",
      "Timestep: 12500\n",
      "Timestep: 12600\n",
      "Timestep: 12700\n",
      "Timestep: 12800\n",
      "Timestep: 12900\n",
      "Timestep: 13000\n",
      "Timestep: 13100\n",
      "Timestep: 13200\n",
      "Timestep: 13300\n",
      "Timestep: 13400\n",
      "Timestep: 13500\n",
      "Timestep: 13600\n",
      "Timestep: 13700\n",
      "Timestep: 13800\n",
      "Timestep: 13900\n",
      "Timestep: 14000\n",
      "Timestep: 14100\n",
      "Timestep: 14200\n",
      "Timestep: 14300\n",
      "Timestep: 14400\n",
      "Timestep: 14500\n",
      "Timestep: 14600\n",
      "Timestep: 14700\n",
      "Timestep: 14800\n",
      "Timestep: 14900\n",
      "Timestep: 15000\n",
      "Timestep: 15100\n",
      "Timestep: 15200\n",
      "Timestep: 15300\n",
      "Timestep: 15400\n",
      "Timestep: 15500\n",
      "Timestep: 15600\n",
      "Timestep: 15700\n",
      "Timestep: 15800\n",
      "Timestep: 15900\n",
      "Timestep: 16000\n",
      "Timestep: 16100\n",
      "Timestep: 16200\n",
      "Timestep: 16300\n",
      "Timestep: 16400\n",
      "Timestep: 16500\n",
      "Timestep: 16600\n",
      "Timestep: 16700\n",
      "Timestep: 16800\n",
      "Timestep: 16900\n",
      "Timestep: 17000\n",
      "Timestep: 17100\n",
      "Timestep: 17200\n",
      "Timestep: 17300\n",
      "Timestep: 17400\n",
      "Timestep: 17500\n",
      "Timestep: 17600\n",
      "Timestep: 17700\n",
      "Timestep: 17800\n",
      "Timestep: 17900\n",
      "Timestep: 18000\n",
      "Timestep: 18100\n",
      "Timestep: 18200\n",
      "Timestep: 18300\n",
      "Timestep: 18400\n",
      "Timestep: 18500\n",
      "Timestep: 18600\n",
      "Timestep: 18700\n",
      "Timestep: 18800\n",
      "Timestep: 18900\n",
      "Timestep: 19000\n",
      "Timestep: 19100\n",
      "Timestep: 19200\n",
      "Timestep: 19300\n",
      "Timestep: 19400\n",
      "Timestep: 19500\n",
      "Timestep: 19600\n",
      "Timestep: 19700\n",
      "Timestep: 19800\n",
      "Timestep: 19900\n",
      "Timestep: 20000\n",
      "Timestep: 20100\n",
      "Timestep: 20200\n",
      "Timestep: 20300\n",
      "Timestep: 20400\n",
      "Timestep: 20500\n",
      "Timestep: 20600\n",
      "Timestep: 20700\n",
      "Timestep: 20800\n",
      "Timestep: 20900\n",
      "Timestep: 21000\n",
      "Timestep: 21100\n",
      "Timestep: 21200\n",
      "Timestep: 21300\n",
      "Timestep: 21400\n",
      "Timestep: 21500\n",
      "Timestep: 21600\n",
      "Timestep: 21700\n",
      "Timestep: 21800\n",
      "Timestep: 21900\n",
      "Timestep: 22000\n",
      "Timestep: 22100\n",
      "Timestep: 22200\n",
      "Timestep: 22300\n",
      "Timestep: 22400\n",
      "Timestep: 22500\n",
      "Timestep: 22600\n",
      "Timestep: 22700\n",
      "Timestep: 22800\n",
      "Timestep: 22900\n",
      "Timestep: 23000\n",
      "Timestep: 23100\n",
      "Timestep: 23200\n",
      "Timestep: 23300\n",
      "Timestep: 23400\n",
      "Timestep: 23500\n",
      "Timestep: 23600\n",
      "Timestep: 23700\n",
      "Timestep: 23800\n",
      "Timestep: 23900\n",
      "Timestep: 24000\n",
      "Timestep: 24100\n",
      "Timestep: 24200\n",
      "Timestep: 24300\n",
      "Timestep: 24400\n",
      "Timestep: 24500\n",
      "Timestep: 24600\n",
      "Timestep: 24700\n",
      "Timestep: 24800\n",
      "Timestep: 24900\n",
      "Timestep: 25000\n",
      "Timestep: 25100\n",
      "Timestep: 25200\n",
      "Timestep: 25300\n",
      "Timestep: 25400\n",
      "Timestep: 25500\n",
      "Timestep: 25600\n",
      "Timestep: 25700\n",
      "Timestep: 25800\n",
      "Timestep: 25900\n",
      "Timestep: 26000\n",
      "Timestep: 26100\n",
      "Timestep: 26200\n",
      "Timestep: 26300\n",
      "Timestep: 26400\n",
      "Timestep: 26500\n",
      "Timestep: 26600\n",
      "Timestep: 26700\n",
      "Timestep: 26800\n",
      "Timestep: 26900\n",
      "Timestep: 27000\n",
      "Timestep: 27100\n",
      "Actual: \n",
      "torch.Size([26100, 8])\n",
      "Predicted: \n",
      "torch.Size([26100, 8])\n",
      "Average Testing Loss: 0.5902854800224304\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "num_samples = 1\n",
    "\n",
    "for i, sample in enumerate(test_dl):\n",
    "    x = sample[0].to(device)\n",
    "    diff = sample[1].to(device)\n",
    "    y = sample[2].to(device)\n",
    "    decoded_map = decode_audio(enc, dec, x, diff)[:y.shape[0], :]\n",
    "    print(\"Actual: \")\n",
    "    print(y.shape)\n",
    "    print(\"Predicted: \")\n",
    "    print(decoded_map.shape)\n",
    "    curr_loss = nn.MSELoss()(y, decoded_map)\n",
    "    test_loss += curr_loss\n",
    "test_loss /= num_samples\n",
    "print(f\"Average Testing Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEvlq_f-kFT2"
   },
   "source": [
    "## Evaluate that bish B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P-zjvlU6kFT3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-22 17:29:20] Epoch: 1\n",
      "files length: 40, index: 23\n",
      "filename: 133178_1\n",
      "[2024-06-22 17:29:21] Current sample 1\n",
      "Requires 2741 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 1 trained successfully!\n",
      "files length: 40, index: 26\n",
      "filename: 697750_1\n",
      "[2024-06-22 17:29:23] Current sample 2\n",
      "Requires 23065 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 2 trained successfully!\n",
      "files length: 40, index: 28\n",
      "filename: 459178_0\n",
      "[2024-06-22 17:29:37] Current sample 3\n",
      "Requires 31054 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 3 trained successfully!\n",
      "files length: 40, index: 27\n",
      "filename: 867457_1\n",
      "[2024-06-22 17:29:56] Current sample 4\n",
      "Requires 46366 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 4 trained successfully!\n",
      "files length: 40, index: 25\n",
      "filename: 806766_0\n",
      "[2024-06-22 17:30:23] Current sample 5\n",
      "Requires 22410 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 5 trained successfully!\n",
      "files length: 40, index: 39\n",
      "filename: 490442_0\n",
      "[2024-06-22 17:30:38] Current sample 6\n",
      "Requires 24366 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 6 trained successfully!\n",
      "files length: 40, index: 14\n",
      "filename: 540207_1\n",
      "[2024-06-22 17:30:55] Current sample 7\n",
      "Requires 9294 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 7 trained successfully!\n",
      "files length: 40, index: 8\n",
      "filename: 387254_0\n",
      "[2024-06-22 17:31:02] Current sample 8\n",
      "Requires 18104 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 8 trained successfully!\n",
      "files length: 40, index: 11\n",
      "filename: 133178_0\n",
      "[2024-06-22 17:31:14] Current sample 9\n",
      "Requires 1326 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 9 trained successfully!\n",
      "files length: 40, index: 19\n",
      "filename: 839741_1\n",
      "[2024-06-22 17:31:16] Current sample 10\n",
      "Requires 10558 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 10 trained successfully!\n",
      "files length: 40, index: 13\n",
      "filename: 697750_2\n",
      "[2024-06-22 17:31:23] Current sample 11\n",
      "Requires 23065 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 11 trained successfully!\n",
      "files length: 40, index: 31\n",
      "filename: 387254_1\n",
      "[2024-06-22 17:31:37] Current sample 12\n",
      "Requires 18104 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 12 trained successfully!\n",
      "files length: 40, index: 22\n",
      "filename: 89744_1\n",
      "[2024-06-22 17:31:50] Current sample 13\n",
      "Requires 14109 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n",
      "Sample 13 trained successfully!\n",
      "files length: 40, index: 2\n",
      "filename: 926189_0\n",
      "[2024-06-22 17:31:59] Current sample 14\n",
      "Requires 3884 iterations\n",
      "Training...1%\n",
      "Training...2%\n",
      "Training...3%\n",
      "Training...4%\n",
      "Training...5%\n",
      "Training...6%\n",
      "Training...7%\n",
      "Training...8%\n",
      "Training...9%\n",
      "Training...10%\n",
      "Training...11%\n",
      "Training...12%\n",
      "Training...13%\n",
      "Training...14%\n",
      "Training...15%\n",
      "Training...16%\n",
      "Training...17%\n",
      "Training...18%\n",
      "Training...19%\n",
      "Training...20%\n",
      "Training...21%\n",
      "Training...22%\n",
      "Training...23%\n",
      "Training...24%\n",
      "Training...25%\n",
      "Training...26%\n",
      "Training...27%\n",
      "Training...28%\n",
      "Training...29%\n",
      "Training...30%\n",
      "Training...31%\n",
      "Training...32%\n",
      "Training...33%\n",
      "Training...34%\n",
      "Training...35%\n",
      "Training...36%\n",
      "Training...37%\n",
      "Training...38%\n",
      "Training...39%\n",
      "Training...40%\n",
      "Training...41%\n",
      "Training...42%\n",
      "Training...43%\n",
      "Training...44%\n",
      "Training...45%\n",
      "Training...46%\n",
      "Training...47%\n",
      "Training...48%\n",
      "Training...49%\n",
      "Training...50%\n",
      "Training...51%\n",
      "Training...52%\n",
      "Training...53%\n",
      "Training...54%\n",
      "Training...55%\n",
      "Training...56%\n",
      "Training...57%\n",
      "Training...58%\n",
      "Training...59%\n",
      "Training...60%\n",
      "Training...61%\n",
      "Training...62%\n",
      "Training...63%\n",
      "Training...64%\n",
      "Training...65%\n",
      "Training...66%\n",
      "Training...67%\n",
      "Training...68%\n",
      "Training...69%\n",
      "Training...70%\n",
      "Training...71%\n",
      "Training...72%\n",
      "Training...73%\n",
      "Training...74%\n",
      "Training...75%\n",
      "Training...76%\n",
      "Training...77%\n",
      "Training...78%\n",
      "Training...79%\n",
      "Training...80%\n",
      "Training...81%\n",
      "Training...82%\n",
      "Training...83%\n",
      "Training...84%\n",
      "Training...85%\n",
      "Training...86%\n",
      "Training...87%\n",
      "Training...88%\n",
      "Training...89%\n",
      "Training...90%\n",
      "Training...91%\n",
      "Training...92%\n",
      "Training...93%\n",
      "Training...94%\n",
      "Training...95%\n",
      "Training...96%\n",
      "Training...97%\n",
      "Training...98%\n",
      "Training...99%\n",
      "Training...100%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m full_dec \u001b[38;5;241m=\u001b[39m Decoder(\u001b[38;5;241m0.4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m a2m_dl \u001b[38;5;241m=\u001b[39m DataLoader(a2m_data, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, batch_sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m full_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma2m_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, encoder, decoder, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     39\u001b[0m     tsprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     curr_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     42\u001b[0m     losshistory\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data, encoder, decoder, encoder_opt, decoder_opt, lossfunc)\u001b[0m\n\u001b[1;32m     17\u001b[0m decoder_outputs, _, _ \u001b[38;5;241m=\u001b[39m decoder(encoder_outputs, encoder_hc, diff, target\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfunc(decoder_outputs, y)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m encoder_opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m decoder_opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/coding/osu-beatmap-generator/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_enc = Encoder(0.4).to(device)\n",
    "full_dec = Decoder(0.4).to(device)\n",
    "a2m_dl = DataLoader(a2m_data, batch_size = None, batch_sampler = None, shuffle = True)\n",
    "\n",
    "full_train_loss = train(a2m_dl, enc, dec, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c25IBt1rci8E"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(full_train_loss)), full_train_loss, 'b', label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4CO4F-EcomA"
   },
   "outputs": [],
   "source": [
    "full_test_loss = 0\n",
    "num_samples = 1\n",
    "\n",
    "for i, sample in enumerate(test_dl):\n",
    "    x = sample[0].to(device)\n",
    "    diff = sample[1].to(device)\n",
    "    y = sample[2].to(device)\n",
    "    decoded_map = decode_audio(enc, dec, x, diff, y)[:y.shape[0]]\n",
    "    print(\"Actual: \")\n",
    "    print(y)\n",
    "    print(\"Predicted: \")\n",
    "    print(decoded_map)\n",
    "    curr_loss = nn.MSELoss()(y, decoded_map)\n",
    "    print(curr_loss)\n",
    "    full_test_loss += curr_loss\n",
    "full_test_loss /= num_samples\n",
    "print(f\"Average Testing Loss: {full_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "osu-beatmap-generator",
   "language": "python",
   "name": "osu-beatmap-generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
